{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOtK3rCTqzIUZCpqVbO9evk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GaborVxxx/tinygpt/blob/main/TinyGTP3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, subprocess, os\n",
        "print(\"CUDA available:\", torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkUU2O5551ed",
        "outputId": "bd7598ae-53ec-4a7b-f061-fd53a03eca5b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 3) Build mixed chat corpus → <A>/<B>/<eot> (working sources only, fixed PersonaChat)\n",
        "# ================================\n",
        "import re, random\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "\n",
        "random.seed(1337)\n",
        "\n",
        "# ---------------- Formatting helpers ----------------\n",
        "ROLE_PREFIX_RE = re.compile(r'^(?:<A>|<B>|Person\\s+[AB]:|[AB]:)\\s*', flags=re.IGNORECASE)\n",
        "_CONTRACTION_RE = re.compile(r\"\\s+'\\s+(m|ve|re|ll|d|s)\\b\", flags=re.IGNORECASE)\n",
        "_ACRONYM_RE     = re.compile(r\"\\b([A-Za-z])\\s*\\.\\s*([A-Za-z])\\b\")\n",
        "_PERSONA_LINE_RE = re.compile(r\"^\\s*(your persona:|partner'?s persona:)\\s*\", flags=re.IGNORECASE)\n",
        "\n",
        "def strip_role_prefixes(s: str) -> str:\n",
        "    return ROLE_PREFIX_RE.sub('', s.strip())\n",
        "\n",
        "def clean_sentence(s: str) -> str:\n",
        "    s = s.strip()\n",
        "    # punctuation spacing\n",
        "    s = re.sub(r\"\\s+([,!.?])\", r\"\\1\", s)\n",
        "    # fix \"I ' m\" -> \"I'm\", etc.\n",
        "    s = _CONTRACTION_RE.sub(r\"'\\1\", s)\n",
        "    # collapse initials \"I . D .\" -> \"I.D.\"\n",
        "    s = _ACRONYM_RE.sub(r\"\\1.\\2\", s)\n",
        "    s = _ACRONYM_RE.sub(r\"\\1.\\2\", s)\n",
        "    return s\n",
        "\n",
        "def sanitize_turns(seq):\n",
        "    \"\"\"Drop persona header lines and dict-like artifacts.\"\"\"\n",
        "    out = []\n",
        "    for x in seq:\n",
        "        if not isinstance(x, str):\n",
        "            continue\n",
        "        x = x.strip()\n",
        "        if not x:\n",
        "            continue\n",
        "        if _PERSONA_LINE_RE.match(x):\n",
        "            continue\n",
        "        # drop obvious dict/list dumps\n",
        "        if x.startswith(\"{\") or x.startswith(\"[\") or \"'candidates':\" in x or '\"candidates\":' in x:\n",
        "            continue\n",
        "        out.append(x)\n",
        "    return out\n",
        "\n",
        "def to_ab_eot(lines):\n",
        "    \"\"\"List[str] -> <bos>\\n<A> ... <eot>\\n<B> ... <eot>\\n<eos> with A/B alternation.\"\"\"\n",
        "    out = []\n",
        "    for i, s in enumerate(lines):\n",
        "        role = \"<A>\" if (i % 2 == 0) else \"<B>\"\n",
        "        utt = clean_sentence(strip_role_prefixes(str(s)))\n",
        "        if not utt:\n",
        "            continue\n",
        "        out.append(f\"{role} {utt} <eot>\")\n",
        "    if not out:\n",
        "        return None\n",
        "    return \"<bos>\\n\" + \"\\n\".join(out) + \"\\n<eos>\"\n",
        "\n",
        "def row_to_dialog_list(row):\n",
        "    \"\"\"Generic extractor for list-like fields (used for DailyDialog/BST).\"\"\"\n",
        "    for key in (\"dialog\",\"dialogs\",\"utterances\",\"texts\",\"conversation\",\"conversations\",\"data\",\"history\",\"Conversation\"):\n",
        "        if key in row:\n",
        "            val = row[key]; break\n",
        "    else:\n",
        "        k0 = next(iter(row.keys())); val = row[k0]\n",
        "    if isinstance(val, list):\n",
        "        if val and isinstance(val[0], str):\n",
        "            return val\n",
        "        if val and isinstance(val[0], dict):\n",
        "            for kk in (\"utterance\",\"text\",\"content\",\"message\",\"msg\"):\n",
        "                if kk in val[0]:\n",
        "                    return [str(u.get(kk, \"\")) for u in val]\n",
        "            return [str(u) for u in val]\n",
        "    return [str(val)]\n",
        "\n",
        "# ---------------- Colab-friendly caps ----------------\n",
        "MAX_TRAIN_PER_DATASET = 25000\n",
        "MAX_VAL_PER_DATASET   = 3000\n",
        "MIN_TURNS = 2          # allow 2-turn dialogs\n",
        "MAX_TURNS = 20\n",
        "\n",
        "def format_dialog_list(seq):\n",
        "    seq = sanitize_turns([s for s in seq if isinstance(s, str)])\n",
        "    if len(seq) < MIN_TURNS:\n",
        "        return None\n",
        "    seq = seq[:MAX_TURNS]\n",
        "    return to_ab_eot(seq)\n",
        "\n",
        "def ensure_tv(ds: Dataset | DatasetDict) -> dict:\n",
        "    \"\"\"Normalize to {'train': Dataset, 'validation': Dataset}.\"\"\"\n",
        "    if isinstance(ds, DatasetDict):\n",
        "        if \"train\" in ds:\n",
        "            train = ds[\"train\"]\n",
        "            valid = ds.get(\"validation\") or ds.get(\"valid\") or ds.get(\"test\")\n",
        "            if valid is None:\n",
        "                split = train.train_test_split(test_size=0.1, seed=1337)\n",
        "                tv = {\"train\": split[\"train\"], \"validation\": split[\"test\"]}\n",
        "            else:\n",
        "                tv = {\"train\": train, \"validation\": valid}\n",
        "        else:\n",
        "            keys = list(ds.keys())\n",
        "            if len(keys) == 1:\n",
        "                split = ds[keys[0]].train_test_split(test_size=0.1, seed=1337)\n",
        "                tv = {\"train\": split[\"train\"], \"validation\": split[\"test\"]}\n",
        "            else:\n",
        "                tv = {\"train\": ds[keys[0]], \"validation\": ds[keys[1]]}\n",
        "    else:\n",
        "        split = ds.train_test_split(test_size=0.1, seed=1337)\n",
        "        tv = {\"train\": split[\"train\"], \"validation\": split[\"test\"]}\n",
        "    print(f\"  -> splits: train={len(tv['train'])}, val={len(tv['validation'])}\")\n",
        "    return tv\n",
        "\n",
        "# ---------------- Loaders (working mirrors only; no remote code) ----------------\n",
        "def load_dailydialog():\n",
        "    print(\"Loading DailyDialog (elricwan/dailydialog)...\")\n",
        "    raw = load_dataset(\"elricwan/dailydialog\")\n",
        "    tv = ensure_tv(raw)\n",
        "    print(\"DailyDialog OK.\")\n",
        "    return tv, None  # None => not pre-extracted\n",
        "\n",
        "def load_blended_skill_talk():\n",
        "    print(\"Loading BlendedSkillTalk (blended_skill_talk)...\")\n",
        "    raw = load_dataset(\"blended_skill_talk\")\n",
        "    def split(ds):\n",
        "        dialogs = []\n",
        "        for r in ds:\n",
        "            if isinstance(r.get(\"dialog\"), list) and len(r[\"dialog\"]) >= 2:\n",
        "                seq = []\n",
        "                for t in r[\"dialog\"]:\n",
        "                    seq.append(t.get(\"text\", \"\") if isinstance(t, dict) else str(t))\n",
        "                if len([s for s in seq if s.strip()]) >= 2:\n",
        "                    dialogs.append(seq); continue\n",
        "            # fallback\n",
        "            msgs = []\n",
        "            if isinstance(r.get(\"previous_utterance\"), str) and r[\"previous_utterance\"].strip():\n",
        "                msgs.append(r[\"previous_utterance\"])\n",
        "            for key in (\"free_messages\", \"guided_messages\"):\n",
        "                val = r.get(key)\n",
        "                if isinstance(val, list):\n",
        "                    for s in val:\n",
        "                        if isinstance(s, dict): s = s.get(\"text\", \"\")\n",
        "                        if isinstance(s, str) and s.strip(): msgs.append(s)\n",
        "            if len(msgs) >= 2:\n",
        "                dialogs.append(msgs)\n",
        "        return dialogs\n",
        "    train = split(raw[\"train\"])\n",
        "    val   = split(raw[\"validation\"])\n",
        "    print(f\"BlendedSkillTalk OK: extracted train={len(train)}, val={len(val)}\")\n",
        "    return {\"train\": train, \"validation\": val}, \"pre-extracted\"\n",
        "\n",
        "def load_persona_parquet():\n",
        "    print(\"Loading PersonaChat parquet mirrors...\")\n",
        "    repos = [\n",
        "        (\"AlekseyKorshuk/persona-chat\", 25000),\n",
        "        (\"Cynaptics/persona-chat\",     25000),\n",
        "    ]\n",
        "    collected = {\"train\": [], \"validation\": []}\n",
        "\n",
        "    def extract_seq_from_row(r):\n",
        "        \"\"\"\n",
        "        Persona-style rows:\n",
        "          - flat: history (list[str]) + label/response (optional)\n",
        "          - nested: utterances: [{history: [...], response/label: str, candidates: [...]}, ...]\n",
        "          - or a single text field with __eou__/__eot__ markers\n",
        "        \"\"\"\n",
        "        # case 1: flattened\n",
        "        hist = r.get(\"history\")\n",
        "        if isinstance(hist, list) and len(hist) >= 1:\n",
        "            seq = [str(x) for x in hist]\n",
        "            reply = r.get(\"response\") or r.get(\"label\")\n",
        "            if isinstance(reply, str) and reply.strip():\n",
        "                seq.append(reply)\n",
        "            return [sanitize_turns(seq)] if len(seq) >= 2 else None\n",
        "\n",
        "        # case 2: nested utterances\n",
        "        uts = r.get(\"utterances\")\n",
        "        if isinstance(uts, list) and uts:\n",
        "            outs = []\n",
        "            for u in uts:\n",
        "                uh = u.get(\"history\")\n",
        "                if isinstance(uh, list) and len(uh) >= 1:\n",
        "                    seq = [str(x) for x in uh]\n",
        "                    reply = u.get(\"response\") or u.get(\"label\")\n",
        "                    if isinstance(reply, str) and reply.strip():\n",
        "                        seq.append(reply)\n",
        "                    seq = sanitize_turns(seq)\n",
        "                    if len(seq) >= 2:\n",
        "                        outs.append(seq)\n",
        "            return outs or None\n",
        "\n",
        "        # case 3: monolithic text with markers\n",
        "        for field in (\"text\",\"dialog\",\"dialogs\",\"conversation\"):\n",
        "            if isinstance(r.get(field), str):\n",
        "                txt = r[field]\n",
        "                parts = [p.strip() for p in re.split(r\"__eou__|__eot__|\\n+\", txt) if p.strip()]\n",
        "                parts = sanitize_turns(parts)\n",
        "                if len(parts) >= 2:\n",
        "                    return [parts]\n",
        "        return None\n",
        "\n",
        "    for repo, cap in repos:\n",
        "        try:\n",
        "            raw = load_dataset(repo)\n",
        "            tv = ensure_tv(raw)\n",
        "            added_train_before = len(collected[\"train\"])\n",
        "            added_val_before   = len(collected[\"validation\"])\n",
        "            for split_name in (\"train\",\"validation\"):\n",
        "                ds = tv.get(split_name)\n",
        "                if ds is None: continue\n",
        "                seqs = []\n",
        "                for r in ds:\n",
        "                    ex = extract_seq_from_row(r)\n",
        "                    if not ex:\n",
        "                        continue\n",
        "                    # ex may be a list of sequences (from nested utterances)\n",
        "                    if ex and isinstance(ex[0], list):\n",
        "                        seqs.extend(ex)\n",
        "                    else:\n",
        "                        seqs.append(ex)\n",
        "                # flatten once more just in case\n",
        "                flat = []\n",
        "                for item in seqs:\n",
        "                    if isinstance(item, list) and item and isinstance(item[0], str):\n",
        "                        flat.append(item)\n",
        "                    elif isinstance(item, list) and item and isinstance(item[0], list):\n",
        "                        flat.extend(item)\n",
        "                random.shuffle(flat)\n",
        "                flat = flat[:cap]\n",
        "                collected[split_name].extend(flat)\n",
        "            print(f\"  Persona OK: {repo} (+{len(collected['train']) - added_train_before} train / +{len(collected['validation']) - added_val_before} val)\")\n",
        "        except Exception as e:\n",
        "            print(f\"  Persona '{repo}' failed, skipping: {e}\")\n",
        "    total_tr, total_v = len(collected[\"train\"]), len(collected[\"validation\"])\n",
        "    if total_tr == 0 and total_v == 0:\n",
        "        print(\"Persona parquet mirrors failed; skipping PersonaChat.\")\n",
        "        return None\n",
        "    print(f\"PersonaChatParquet OK: total train={total_tr}, val={total_v}\")\n",
        "    return {\"train\": collected[\"train\"], \"validation\": collected[\"validation\"]}, \"pre-extracted\"\n",
        "\n",
        "# ---------------- Pull sources ----------------\n",
        "sources = []\n",
        "\n",
        "dd  = load_dailydialog()\n",
        "bst = load_blended_skill_talk()\n",
        "pc  = load_persona_parquet()\n",
        "\n",
        "if dd:  sources.append((\"DailyDialog\", dd[0],  dd[1]))\n",
        "if bst: sources.append((\"BlendedSkillTalk\", bst[0], bst[1]))\n",
        "if pc:  sources.append((\"PersonaChatParquet\",  pc[0],  pc[1]))\n",
        "\n",
        "# ---------------- Format, cap, dedup, validate ----------------\n",
        "def format_split(obj, split_name, kind):\n",
        "    if kind == \"pre-extracted\":\n",
        "        seqs = obj[split_name]\n",
        "    else:\n",
        "        ds = obj[split_name]\n",
        "        seqs = [row_to_dialog_list(r) for r in ds]\n",
        "    fixed = []\n",
        "    for x in seqs:\n",
        "        if not x: continue\n",
        "        if isinstance(x, list): fixed.append([str(xx) for xx in x])\n",
        "        else: fixed.append([str(x)])\n",
        "    out = []\n",
        "    for seq in fixed:\n",
        "        s = format_dialog_list(seq)\n",
        "        if s: out.append(s)\n",
        "    return out\n",
        "\n",
        "train_texts_all, val_texts_all = [], []\n",
        "for name, ds_or_lists, kind in sources:\n",
        "    print(f\"\\n--- Formatting {name} ---\")\n",
        "    t = format_split(ds_or_lists, \"train\", kind)\n",
        "    v = format_split(ds_or_lists, \"validation\", kind)\n",
        "    random.shuffle(t); t = t[:MAX_TRAIN_PER_DATASET]\n",
        "    random.shuffle(v); v = v[:MAX_VAL_PER_DATASET]\n",
        "    print(f\"{name}: formatted train={len(t)}, val={len(v)}\")\n",
        "    train_texts_all.extend(t)\n",
        "    val_texts_all.extend(v)\n",
        "\n",
        "# Dedup\n",
        "def norm_for_dedup(s: str) -> str:\n",
        "    return re.sub(r'\\s+', ' ', s.strip().lower())\n",
        "def dedup_keep_first(texts):\n",
        "    seen = set(); out = []\n",
        "    for x in texts:\n",
        "        k = norm_for_dedup(x)\n",
        "        if k in seen: continue\n",
        "        seen.add(k); out.append(x)\n",
        "    return out\n",
        "\n",
        "train_texts = dedup_keep_first(train_texts_all)\n",
        "val_texts   = dedup_keep_first(val_texts_all)\n",
        "random.shuffle(train_texts)\n",
        "random.shuffle(val_texts)\n",
        "\n",
        "print(\"\\n=== Mixed corpus sizes (pre-validate) ===\")\n",
        "print(f\"train: {len(train_texts)}  |  val: {len(val_texts)}\")\n",
        "\n",
        "# Validate (<bos>/<eos>, strict A/B alternation, per-line <eot>)\n",
        "LINE_RE = re.compile(r'^(<A>|<B>)\\s.+\\s<eot>$')\n",
        "def validate_dialog(text: str):\n",
        "    errs = []\n",
        "    if not text.startswith(\"<bos>\\n\"): errs.append(\"missing <bos> at start\")\n",
        "    if not text.rstrip().endswith(\"<eos>\"): errs.append(\"missing <eos> at end\")\n",
        "    body = [ln for ln in text.splitlines() if ln not in (\"<bos>\", \"<eos>\")]\n",
        "    if not body:\n",
        "        errs.append(\"empty dialog body\");\n",
        "        return errs\n",
        "    prev_speaker = None\n",
        "    for i, ln in enumerate(body):\n",
        "        m = LINE_RE.match(ln)\n",
        "        if not m:\n",
        "            errs.append(f\"line {i} malformed: {ln[:120]}\")\n",
        "            continue\n",
        "        spk = m.group(1)\n",
        "        if prev_speaker == spk:\n",
        "            errs.append(f\"consecutive same speaker at lines {i-1},{i}\")\n",
        "        prev_speaker = spk\n",
        "    return errs\n",
        "\n",
        "def filter_bad(texts):\n",
        "    keep = []; bad = 0\n",
        "    for t in texts:\n",
        "        e = validate_dialog(t)\n",
        "        if e: bad += 1\n",
        "        else: keep.append(t)\n",
        "    return keep, bad\n",
        "\n",
        "train_texts, bad_train = filter_bad(train_texts)\n",
        "val_texts,   bad_val   = filter_bad(val_texts)\n",
        "\n",
        "print(f\"\\nValidation — dropped: train {bad_train}, val {bad_val}\")\n",
        "print(f\"Final counts — train {len(train_texts)}, val {len(val_texts)}\")\n",
        "\n",
        "if train_texts:\n",
        "    print(\"\\nPreview (formatted):\\n\", train_texts[0][:500])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iy6_NYmEvKgA",
        "outputId": "d2b49d4e-98fd-4713-d67d-fbd7de4842ca"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading DailyDialog (elricwan/dailydialog)...\n",
            "  -> splits: train=11806, val=1312\n",
            "DailyDialog OK.\n",
            "Loading BlendedSkillTalk (blended_skill_talk)...\n",
            "BlendedSkillTalk OK: extracted train=4819, val=1009\n",
            "Loading PersonaChat parquet mirrors...\n",
            "  -> splits: train=17878, val=1000\n",
            "  Persona OK: AlekseyKorshuk/persona-chat (+25000 train / +6801 val)\n",
            "  -> splits: train=18000, val=2000\n",
            "  Persona OK: Cynaptics/persona-chat (+0 train / +0 val)\n",
            "PersonaChatParquet OK: total train=25000, val=6801\n",
            "\n",
            "--- Formatting DailyDialog ---\n",
            "DailyDialog: formatted train=11806, val=1312\n",
            "\n",
            "--- Formatting BlendedSkillTalk ---\n",
            "BlendedSkillTalk: formatted train=4819, val=1009\n",
            "\n",
            "--- Formatting PersonaChatParquet ---\n",
            "PersonaChatParquet: formatted train=25000, val=3000\n",
            "\n",
            "=== Mixed corpus sizes (pre-validate) ===\n",
            "train: 40935  |  val: 5312\n",
            "\n",
            "Validation — dropped: train 0, val 0\n",
            "Final counts — train 40935, val 5312\n",
            "\n",
            "Preview (formatted):\n",
            " <bos>\n",
            "<A> What a beautiful view, my sweetheart! <eot>\n",
            "<B> It sure is.The Grand Canyon is truly masterpiece. No man could ever make anything like this. <eot>\n",
            "<A> What is that below? <eot>\n",
            "<B> It is the Colorado River. You will go down the river in a boat if you wish. <eot>\n",
            "<A> No, thank you.It looks a bit too dangerous for me. <eot>\n",
            "<B> There is a beautiful state park here. I can see why so many people come out here to camp. <eot>\n",
            "<A> Yes, there is this beautiful view. you may go fishing, camping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbxiiKFQcg0l",
        "outputId": "d2e03075-5fcc-4139-d652-d09b2c3b850d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n",
            "CPU detected → enabling CPU-Lite mode\n",
            "Vocab size: 8000 {'<bos>': 4, '<eos>': 5, '<A>': 6, '<B>': 7, '<eot>': 8}\n",
            "Encoded tokens: 6418580 909924\n",
            "Batches: 18750 2343\n",
            "Total parameters: 4.22M\n",
            "step 50/1200 lr 1.50e-04 loss 7.332\n",
            "step 100/1200 lr 3.00e-04 loss 5.277\n",
            "step 150/1200 lr 4.50e-04 loss 4.395\n",
            "step 200/1200 lr 6.00e-04 loss 4.245\n",
            "[eval 200] val_loss 4.316  (+158.3s)\n",
            "✓ saved /content/tinygpt_best.pt\n",
            "step 250/1200 lr 5.97e-04 loss 4.297\n",
            "step 300/1200 lr 5.87e-04 loss 3.873\n",
            "step 350/1200 lr 5.71e-04 loss 3.868\n",
            "step 400/1200 lr 5.48e-04 loss 3.712\n",
            "[eval 400] val_loss 3.924  (+158.9s)\n",
            "✓ saved /content/tinygpt_best.pt\n",
            "step 450/1200 lr 5.21e-04 loss 3.884\n",
            "step 500/1200 lr 4.89e-04 loss 3.308\n",
            "step 550/1200 lr 4.53e-04 loss 3.262\n",
            "step 600/1200 lr 4.13e-04 loss 3.505\n",
            "[eval 600] val_loss 3.777  (+158.0s)\n",
            "✓ saved /content/tinygpt_best.pt\n",
            "step 650/1200 lr 3.72e-04 loss 3.861\n",
            "step 700/1200 lr 3.30e-04 loss 3.595\n",
            "step 750/1200 lr 2.88e-04 loss 3.844\n",
            "step 800/1200 lr 2.47e-04 loss 3.539\n",
            "[eval 800] val_loss 3.694  (+158.6s)\n",
            "✓ saved /content/tinygpt_best.pt\n",
            "step 850/1200 lr 2.07e-04 loss 3.100\n",
            "step 900/1200 lr 1.71e-04 loss 3.516\n",
            "step 950/1200 lr 1.39e-04 loss 3.657\n",
            "step 1000/1200 lr 1.12e-04 loss 3.479\n",
            "[eval 1000] val_loss 3.638  (+159.7s)\n",
            "✓ saved /content/tinygpt_best.pt\n",
            "step 1050/1200 lr 8.94e-05 loss 3.637\n",
            "step 1100/1200 lr 7.32e-05 loss 3.445\n",
            "step 1150/1200 lr 6.33e-05 loss 3.207\n",
            "step 1200/1200 lr 6.00e-05 loss 3.279\n",
            "[eval 1200] val_loss 3.618  (+157.8s)\n",
            "✓ saved /content/tinygpt_best.pt\n",
            "Loaded best checkpoint.\n",
            "You: Hi, how are you?\n",
            "Bot: I'm great. I'm good, but I'm fine. I love to buy the best dogs.\n",
            "----\n",
            "You: What's your favorite movie?\n",
            "Bot: I know. It's a new of the best time with my husband. Do you like to go in the most year.\n"
          ]
        }
      ],
      "source": [
        "# ================================\n",
        "# 2) Tokenize, encode, train TinyGPT (GPU full / CPU-lite)\n",
        "#  — expects train_texts / val_texts from Cell 3\n",
        "# ================================\n",
        "!pip -q install sentencepiece\n",
        "\n",
        "import os, math, time, random, re, numpy as np\n",
        "import torch, torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import sentencepiece as spm\n",
        "\n",
        "assert 'train_texts' in globals() and 'val_texts' in globals(), \"Run Cell 3 first.\"\n",
        "\n",
        "# ---- Device + seeds\n",
        "torch.manual_seed(1337); random.seed(1337)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "if hasattr(torch, \"set_float32_matmul_precision\"):\n",
        "    torch.set_float32_matmul_precision(\"high\")\n",
        "\n",
        "# ---- Hyperparams (GPU full defaults)\n",
        "block_size   = 128\n",
        "batch_size   = 64\n",
        "base_lr      = 3e-4\n",
        "warmup_steps = 1_000\n",
        "max_steps    = 20_000\n",
        "eval_every   = 1_000\n",
        "\n",
        "# ---- CPU-lite fallback so it actually finishes\n",
        "if device == \"cpu\":\n",
        "    print(\"CPU detected → enabling CPU-Lite mode\")\n",
        "    block_size   = 64\n",
        "    batch_size   = 16\n",
        "    base_lr      = 6e-4\n",
        "    warmup_steps = 200\n",
        "    max_steps    = 1_200\n",
        "    eval_every   = 200\n",
        "    # cap total tokens to keep things moving\n",
        "    _TARGET_TOKS_TRAIN = 1_200_000\n",
        "    _TARGET_TOKS_VAL   =   150_000\n",
        "\n",
        "# ================================\n",
        "# Tokenizer (Unigram, 8k, byte fallback)\n",
        "# ================================\n",
        "corpus_path = \"/content/mixed_dialog_corpus.txt\"\n",
        "with open(corpus_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.writelines(t + \"\\n\" for t in train_texts)\n",
        "\n",
        "tok_path = \"/content/spm_chat_8k.model\"\n",
        "!rm -f /content/spm_chat_8k.model /content/spm_chat_8k.vocab\n",
        "\n",
        "spm.SentencePieceTrainer.Train(\n",
        "    input=corpus_path,\n",
        "    model_prefix=\"/content/spm_chat_8k\",\n",
        "    vocab_size=8000,\n",
        "    model_type=\"unigram\",\n",
        "    byte_fallback=True,\n",
        "    normalization_rule_name=\"nmt_nfkc\",\n",
        "    pad_id=0, unk_id=1, bos_id=2, eos_id=3,\n",
        "    user_defined_symbols=[\"<bos>\",\"<eos>\",\"<A>\",\"<B>\",\"<eot>\"]\n",
        ")\n",
        "sp = spm.SentencePieceProcessor(model_file=tok_path)\n",
        "vocab_size = sp.vocab_size()\n",
        "pad_id, unk_id, bos_id, eos_id = 0, 1, 2, 3\n",
        "print(\"Vocab size:\", vocab_size, {t: sp.piece_to_id(t) for t in [\"<bos>\",\"<eos>\",\"<A>\",\"<B>\",\"<eot>\"]})\n",
        "\n",
        "# ================================\n",
        "# Encode → chunk dataset\n",
        "# ================================\n",
        "def encode_texts(texts):\n",
        "    return np.array([tid for t in texts for tid in sp.encode(t, out_type=int)], dtype=np.int32)\n",
        "\n",
        "train_ids = encode_texts(train_texts)\n",
        "val_ids   = encode_texts(val_texts)\n",
        "print(\"Encoded tokens:\", len(train_ids), len(val_ids))\n",
        "\n",
        "# CPU-lite: subsample tokens before making datasets\n",
        "if device == \"cpu\":\n",
        "    if len(train_ids) > _TARGET_TOKS_TRAIN + 1:\n",
        "        train_ids = train_ids[:_TARGET_TOKS_TRAIN + 1]\n",
        "    if len(val_ids) > _TARGET_TOKS_VAL + 1:\n",
        "        val_ids = val_ids[:_TARGET_TOKS_VAL + 1]\n",
        "\n",
        "class LMChunkDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, ids, block_size):\n",
        "        L = (len(ids) - 1) // block_size\n",
        "        self.input  = torch.tensor(ids[:L*block_size],    dtype=torch.long).view(L, block_size)\n",
        "        self.target = torch.tensor(ids[1:L*block_size+1], dtype=torch.long).view(L, block_size)\n",
        "    def __len__(self): return self.input.size(0)\n",
        "    def __getitem__(self, i): return self.input[i], self.target[i]\n",
        "\n",
        "train_ds = LMChunkDataset(train_ids, block_size)\n",
        "val_ds   = LMChunkDataset(val_ids, block_size)\n",
        "\n",
        "num_workers = 0 if device == \"cpu\" else 2\n",
        "pin_memory  = (device == \"cuda\")\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_ds, batch_size=batch_size, shuffle=True, drop_last=True,\n",
        "    num_workers=num_workers, pin_memory=pin_memory, persistent_workers=(num_workers>0)\n",
        ")\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_ds, batch_size=batch_size, shuffle=False, drop_last=True,\n",
        "    num_workers=num_workers, pin_memory=pin_memory, persistent_workers=(num_workers>0)\n",
        ")\n",
        "print(\"Batches:\", len(train_ds), len(val_ds))\n",
        "\n",
        "# ================================\n",
        "# TinyGPT (GPU full vs CPU-Lite sizes)\n",
        "# ================================\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, d_model=256, n_heads=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0\n",
        "        self.n_heads = n_heads\n",
        "        self.d_head  = d_model // n_heads\n",
        "        self.qkv   = nn.Linear(d_model, 3*d_model, bias=False)\n",
        "        self.proj  = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.attn_drop  = nn.Dropout(dropout)\n",
        "        self.resid_drop = nn.Dropout(dropout)\n",
        "        self.register_buffer(\"mask\", None, persistent=False)\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        if (self.mask is None) or (self.mask.size(-1) < T):\n",
        "            self.mask = torch.tril(torch.ones(T, T, device=x.device)).view(1,1,T,T)\n",
        "        qkv = self.qkv(x); q, k, v = qkv.split(C, dim=2)\n",
        "        def split(t): return t.view(B, T, self.n_heads, self.d_head).transpose(1,2)\n",
        "        q, k, v = map(split, (q, k, v))\n",
        "        att = (q @ k.transpose(-2, -1)) / (self.d_head ** 0.5)\n",
        "        att = att.masked_fill(self.mask[:, :, :T, :T] == 0, float(\"-inf\"))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attn_drop(att)\n",
        "        y = att @ v\n",
        "        y = y.transpose(1,2).contiguous().view(B, T, C)\n",
        "        y = self.resid_drop(self.proj(y))\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, d_model=256, d_ff=1024, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "    def forward(self, x): return self.drop(self.fc2(self.act(self.fc1(x))))\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, d_model=256, n_heads=4, d_ff=1024, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.attn = CausalSelfAttention(d_model, n_heads, dropout)\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "        self.mlp  = MLP(d_model, d_ff, dropout)\n",
        "        self.drop_res = nn.Dropout(dropout)\n",
        "    def forward(self, x):\n",
        "        x = x + self.drop_res(self.attn(self.ln1(x)))\n",
        "        x = x + self.drop_res(self.mlp(self.ln2(x)))\n",
        "        return x\n",
        "\n",
        "class TinyGPT(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=256, n_heads=4, d_ff=1024, n_layers=5, max_seq_len=128, dropout=0.1, pad_id=0):\n",
        "        super().__init__()\n",
        "        self.token_embed = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n",
        "        self.pos_embed   = nn.Embedding(max_seq_len, d_model)\n",
        "        self.drop  = nn.Dropout(dropout)\n",
        "        self.blocks = nn.ModuleList([Block(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)])\n",
        "        self.ln_f  = nn.LayerNorm(d_model)\n",
        "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
        "        self.lm_head.weight = self.token_embed.weight\n",
        "        self.apply(self._init)\n",
        "    @staticmethod\n",
        "    def _init(m):\n",
        "        if isinstance(m, (nn.Linear, nn.Embedding)):\n",
        "            nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
        "        if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "            nn.init.zeros_(m.bias)\n",
        "    def forward(self, idx):\n",
        "        B, T = idx.size()\n",
        "        pos = torch.arange(0, T, device=idx.device).unsqueeze(0)\n",
        "        x = self.token_embed(idx) + self.pos_embed(pos)\n",
        "        x = self.drop(x)\n",
        "        for blk in self.blocks: x = blk(x)\n",
        "        x = self.ln_f(x)\n",
        "        return self.lm_head(x)\n",
        "\n",
        "# choose size by device\n",
        "if device == \"cpu\":\n",
        "    model_cfg = dict(d_model=224, n_heads=4, d_ff=896, n_layers=4, max_seq_len=block_size)  # ~3.8–4.2M params\n",
        "else:\n",
        "    model_cfg = dict(d_model=256, n_heads=4, d_ff=1024, n_layers=5, max_seq_len=block_size)\n",
        "model = TinyGPT(vocab_size=vocab_size, pad_id=pad_id, **model_cfg).to(device)\n",
        "print(f\"Total parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")\n",
        "\n",
        "# ================================\n",
        "# Train (AdamW, warmup+cosine, AMP, grad clip)\n",
        "# ================================\n",
        "def get_lr(step, warmup, max_steps, base_lr):\n",
        "    if step < warmup: return base_lr * step / max(1, warmup)\n",
        "    progress = (step - warmup) / max(1, (max_steps - warmup))\n",
        "    return 0.1*base_lr + 0.9*base_lr * 0.5 * (1 + math.cos(math.pi * progress))\n",
        "\n",
        "decay, no_decay = [], []\n",
        "for n,p in model.named_parameters():\n",
        "    if not p.requires_grad: continue\n",
        "    if n.endswith(\"bias\") or \"ln\" in n.lower() or \"layernorm\" in n.lower():\n",
        "        no_decay.append(p)\n",
        "    else:\n",
        "        decay.append(p)\n",
        "optimizer = torch.optim.AdamW(\n",
        "    [{\"params\": decay,    \"weight_decay\": 0.05},\n",
        "     {\"params\": no_decay, \"weight_decay\": 0.00}],\n",
        "    lr=base_lr, betas=(0.9, 0.95)\n",
        ")\n",
        "\n",
        "amp_dtype = torch.bfloat16 if (device==\"cuda\" and torch.cuda.is_bf16_supported()) else torch.float16\n",
        "scaler = torch.amp.GradScaler(\"cuda\", enabled=(device==\"cuda\"))\n",
        "\n",
        "@torch.no_grad()\n",
        "def run_eval():\n",
        "    model.eval(); losses = []\n",
        "    for xb, yb in val_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        with torch.amp.autocast(\"cuda\", dtype=amp_dtype, enabled=(device==\"cuda\")):\n",
        "            logits = model(xb)\n",
        "            loss = F.cross_entropy(logits.view(-1, vocab_size), yb.view(-1), ignore_index=pad_id)\n",
        "        losses.append(loss.item())\n",
        "    model.train()\n",
        "    return sum(losses)/len(losses) if losses else float(\"nan\")\n",
        "\n",
        "ckpt_path = \"/content/tinygpt_best.pt\"\n",
        "best_val = float(\"inf\"); global_step = 0; t0 = time.time()\n",
        "model.train()\n",
        "\n",
        "for epoch in range(999999):\n",
        "    for xb, yb in train_loader:\n",
        "        global_step += 1\n",
        "        lr = get_lr(global_step, warmup_steps, max_steps, base_lr)\n",
        "        for pg in optimizer.param_groups: pg[\"lr\"] = lr\n",
        "\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with torch.amp.autocast(\"cuda\", dtype=amp_dtype, enabled=(device==\"cuda\")):\n",
        "            logits = model(xb)\n",
        "            loss = F.cross_entropy(logits.view(-1, vocab_size), yb.view(-1), ignore_index=pad_id)\n",
        "        scaler.scale(loss).backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        scaler.step(optimizer); scaler.update()\n",
        "\n",
        "        if global_step % 50 == 0:\n",
        "            print(f\"step {global_step}/{max_steps} lr {lr:.2e} loss {loss.item():.3f}\")\n",
        "\n",
        "        if global_step % eval_every == 0:\n",
        "            val_loss = run_eval()\n",
        "            print(f\"[eval {global_step}] val_loss {val_loss:.3f}  (+{time.time()-t0:.1f}s)\")\n",
        "            t0 = time.time()\n",
        "            if val_loss < best_val:\n",
        "                best_val = val_loss\n",
        "                torch.save({\"model\": model.state_dict(),\n",
        "                            \"config\": {**model_cfg,\n",
        "                                       \"vocab_size\": vocab_size,\n",
        "                                       \"block_size\": block_size,\n",
        "                                       \"tokenizer\": tok_path}},\n",
        "                           ckpt_path)\n",
        "                print(\"✓ saved\", ckpt_path)\n",
        "\n",
        "        if global_step >= max_steps: break\n",
        "    if global_step >= max_steps: break\n",
        "\n",
        "# ================================\n",
        "# Sampling (top-k + top-p + sign-aware rep penalty)\n",
        "# ================================\n",
        "def detok_cleanup(txt: str) -> str:\n",
        "    return (txt.replace(\" ,\", \",\").replace(\" .\", \".\").replace(\" !\", \"!\")\n",
        "              .replace(\" ?\", \"?\").replace(\" ' s\",\"'s\").replace(\" ' m\",\"'m\")\n",
        "              .replace(\" ' ve\",\"'ve\").replace(\" ' re\",\"'re\")\n",
        "              .replace(\" ' d\",\"'d\").replace(\" ' ll\",\"'ll\").replace(\" n't\",\"n't\")).strip()\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample(model, sp, prompt,\n",
        "           max_new_tokens=120, temperature=0.65, top_p=0.90, top_k=50,\n",
        "           min_tokens_before_stop=24, repetition_penalty=1.15, penalty_ctx=80,\n",
        "           include_prompt=False):\n",
        "    model.eval()\n",
        "    seed = \"<bos>\\n<A> \" + prompt.strip() + \" <eot>\\n<B> \"\n",
        "    x = torch.tensor(sp.encode(seed, out_type=int), dtype=torch.long, device=device)[None, ...]\n",
        "    start_len = x.size(1)\n",
        "    STOP_STRINGS = [\"<eot>\", \"<eos>\", \"<bos>\", \"<A>\", \"<B>\"]\n",
        "    STOP_SEQS = [sp.encode(s, out_type=int) for s in STOP_STRINGS]\n",
        "    FIRST_TOKENS = {seq[0] for seq in STOP_SEQS if len(seq) > 0}\n",
        "\n",
        "    def ends_with(seq, suffix):\n",
        "        L = len(suffix); return L > 0 and len(seq) >= L and seq[-L:] == suffix\n",
        "    def find_first_stop(gen_ids):\n",
        "        N = len(gen_ids); cut = N\n",
        "        for i in range(N):\n",
        "            for s in STOP_SEQS:\n",
        "                L = len(s)\n",
        "                if L and i+L <= N and gen_ids[i:i+L] == s:\n",
        "                    cut = min(cut, i)\n",
        "        return cut\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        if x.size(1) > block_size: x = x[:, -block_size:]\n",
        "        with torch.amp.autocast(\"cuda\", dtype=amp_dtype, enabled=(device==\"cuda\")):\n",
        "            logits = model(x)[:, -1, :]\n",
        "\n",
        "        # temperature\n",
        "        logits = logits / max(1e-8, temperature)\n",
        "\n",
        "        # sign-aware repetition penalty\n",
        "        if repetition_penalty and penalty_ctx > 0:\n",
        "            recent = x[0, max(0, x.size(1) - penalty_ctx):].tolist()\n",
        "            for t in set(recent):\n",
        "                if logits[0, t] > 0: logits[0, t] /= repetition_penalty\n",
        "                else:                logits[0, t] *= repetition_penalty\n",
        "\n",
        "        # block starting stop seq before min length\n",
        "        gen_len = x.size(1) - start_len\n",
        "        if gen_len < min_tokens_before_stop:\n",
        "            for t0 in FIRST_TOKENS: logits[0, t0] = -float(\"inf\")\n",
        "\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "        # top-k\n",
        "        if top_k is not None and top_k > 0:\n",
        "            topk_vals, topk_idx = torch.topk(probs, k=min(top_k, probs.size(-1)))\n",
        "            mask = torch.ones_like(probs, dtype=torch.bool); mask.scatter_(1, topk_idx, False)\n",
        "            probs[mask] = 0\n",
        "\n",
        "        # top-p\n",
        "        sorted_probs, sorted_idx = torch.sort(probs, descending=True)\n",
        "        cum = torch.cumsum(sorted_probs, dim=-1)\n",
        "        mask = cum > top_p; mask[..., 0] = False\n",
        "        sorted_probs[mask] = 0\n",
        "        sorted_probs /= sorted_probs.sum(dim=-1, keepdim=True)\n",
        "\n",
        "        next_id = sorted_idx.gather(-1, torch.multinomial(sorted_probs, 1))\n",
        "        x = torch.cat([x, next_id], dim=1)\n",
        "\n",
        "        # early stop\n",
        "        gen_ids = x[0, start_len:].tolist()\n",
        "        if gen_len + 1 >= min_tokens_before_stop and any(ends_with(gen_ids, s) for s in STOP_SEQS):\n",
        "            break\n",
        "\n",
        "    gen_ids = x[0, start_len:].tolist()\n",
        "    gen_ids = gen_ids[:find_first_stop(gen_ids)]\n",
        "    txt = detok_cleanup(sp.decode(gen_ids))\n",
        "    return f\"You: {prompt.strip()}\\nBot: {txt}\" if include_prompt else txt\n",
        "\n",
        "# ---- Quick samples (if a checkpoint exists)\n",
        "ckpt_path = \"/content/tinygpt_best.pt\"\n",
        "if os.path.exists(ckpt_path):\n",
        "    sd = torch.load(ckpt_path, map_location=device)\n",
        "    model.load_state_dict(sd[\"model\"])\n",
        "    print(\"Loaded best checkpoint.\")\n",
        "\n",
        "print(sample(model, sp, \"Hi, how are you?\", max_new_tokens=80, include_prompt=True))\n",
        "print(\"----\")\n",
        "print(sample(model, sp, \"What's your favorite movie?\", max_new_tokens=80, include_prompt=True))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# Chat REPL for <A>/<B>/<eot> (no cut-offs, anti-repeat, best-of)\n",
        "# ============================\n",
        "import os, re, torch, sentencepiece as spm\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# --- Paths\n",
        "ckpt_path = next((p for p in [\n",
        "    \"/content/tinygpt_best.pt\",\n",
        "    \"/kaggle/working/tinygpt_best.pt\",\n",
        "] if os.path.exists(p)), None)\n",
        "assert ckpt_path, \"Missing checkpoint (e.g. /content/tinygpt_best.pt).\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "try:\n",
        "    amp_dtype = torch.bfloat16 if (device==\"cuda\" and torch.cuda.is_bf16_supported()) else torch.float16\n",
        "except AttributeError:\n",
        "    amp_dtype = torch.float16\n",
        "\n",
        "# --- Load ckpt + tokenizer\n",
        "ckpt = torch.load(ckpt_path, map_location=device)\n",
        "cfg  = ckpt.get(\"config\", {})\n",
        "\n",
        "tok_candidates = [cfg.get(\"tokenizer\"), \"/content/spm_chat_8k.model\", \"/content/spm_dd_4k.model\"]\n",
        "tok_candidates = [p for p in tok_candidates if p]\n",
        "sp_model_path = next((p for p in tok_candidates if os.path.exists(p)), None)\n",
        "assert sp_model_path, f\"Tokenizer .model not found. Looked for: {tok_candidates}\"\n",
        "\n",
        "sp = spm.SentencePieceProcessor(model_file=sp_model_path)\n",
        "ID = {k: sp.piece_to_id(k) for k in (\"<bos>\",\"<eos>\",\"<A>\",\"<B>\",\"<eot>\")}\n",
        "\n",
        "print(\"Device:\", device)\n",
        "print(\"Loaded tokenizer:\", sp_model_path)\n",
        "print(\"Special IDs:\", ID)\n",
        "\n",
        "# --- Model config\n",
        "vocab_size = cfg.get(\"vocab_size\", sp.vocab_size())\n",
        "pad_id     = 0\n",
        "ctx_len    = cfg.get(\"block_size\", cfg.get(\"max_seq_len\", 128))\n",
        "model_cfg  = dict(\n",
        "    d_model = cfg.get(\"d_model\", 256),\n",
        "    n_heads = cfg.get(\"n_heads\", 4),\n",
        "    d_ff    = cfg.get(\"d_ff\", 1024),\n",
        "    n_layers= cfg.get(\"n_layers\", 5),\n",
        ")\n",
        "\n",
        "# TinyGPT must be defined in the training cell\n",
        "try:\n",
        "    TinyGPT\n",
        "except NameError as e:\n",
        "    raise RuntimeError(\"Run the training cell that defines TinyGPT before this REPL.\") from e\n",
        "\n",
        "model = TinyGPT(\n",
        "    vocab_size=vocab_size,\n",
        "    d_model=model_cfg[\"d_model\"],\n",
        "    n_heads=model_cfg[\"n_heads\"],\n",
        "    d_ff=model_cfg[\"d_ff\"],\n",
        "    n_layers=model_cfg[\"n_layers\"],\n",
        "    max_seq_len=ctx_len,\n",
        "    dropout=0.1, pad_id=pad_id\n",
        ").to(device)\n",
        "model.load_state_dict(ckpt[\"model\"], strict=True)\n",
        "model.eval()\n",
        "print(f\"Loaded model. Params={sum(p.numel() for p in model.parameters())/1e6:.2f}M | Context len={ctx_len}\")\n",
        "\n",
        "# ---------- Helpers ----------\n",
        "_CONTRACTIONS = [\n",
        "    (r\"\\b(I|you|he|she|it|we|they)\\s+'\\s*m\\b\", r\"\\1'm\"),\n",
        "    (r\"\\b(I|you|he|she|it|we|they)\\s+'\\s*re\\b\", r\"\\1're\"),\n",
        "    (r\"\\b(I|you|he|she|it|we|they)\\s+'\\s*ve\\b\", r\"\\1've\"),\n",
        "    (r\"\\b(should|could|would)\\s+'\\s*ve\\b\", r\"\\1've\"),\n",
        "    (r\"\\b(\\w)\\s+'\\s*s\\b\", r\"\\1's\"),\n",
        "    (r\"\\b(\\w)\\s+'\\s*d\\b\", r\"\\1'd\"),\n",
        "    (r\"\\b(\\w)\\s+'\\s*ll\\b\", r\"\\1'll\"),\n",
        "    (r\"\\b(n)\\s+'\\s*t\\b\", r\"\\1't\"),\n",
        "]\n",
        "def detok_cleanup(txt: str) -> str:\n",
        "    txt = (txt.replace(\" ,\", \",\").replace(\" .\", \".\").replace(\" !\", \"!\")\n",
        "              .replace(\" ?\", \"?\").replace(\"  \", \" \").strip())\n",
        "    for pat, rep in _CONTRACTIONS:\n",
        "        txt = re.sub(pat, rep, txt, flags=re.IGNORECASE)\n",
        "    return txt\n",
        "\n",
        "MAX_CTX_TOKENS = ctx_len\n",
        "# Reserve half the context for generation, but at least 16 and at most 64 tokens.\n",
        "RESERVED_GEN_TOKENS = max(16, min(64, ctx_len // 2))\n",
        "\n",
        "def build_seed_from_history(history, user_msg):\n",
        "    \"\"\"history: list[(user, bot)]\"\"\"\n",
        "    def convo(turns, last_user):\n",
        "        lines = []\n",
        "        for u, b in turns:\n",
        "            if u: lines.append(f\"<A> {u.strip()} <eot>\")\n",
        "            if b: lines.append(f\"<B> {b.strip()} <eot>\")\n",
        "        lines.append(f\"<A> {last_user.strip()} <eot>\")\n",
        "        return \"<bos>\\n\" + \"\\n\".join(lines) + \"\\n<B> \"\n",
        "    seed = convo(history, user_msg)\n",
        "    ids  = sp.encode(seed, out_type=int)\n",
        "    # Trim oldest history to keep room for generation\n",
        "    while len(ids) > (MAX_CTX_TOKENS - RESERVED_GEN_TOKENS) and history:\n",
        "        history.pop(0)\n",
        "        seed = convo(history, user_msg)\n",
        "        ids  = sp.encode(seed, out_type=int)\n",
        "    return seed\n",
        "\n",
        "# --- Candidate scoring (for best-of)\n",
        "def _repetition_score(ids, n=3):\n",
        "    if len(ids) < n: return 0.0\n",
        "    seen, reps = set(), 0\n",
        "    for i in range(len(ids)-n+1):\n",
        "        ng = tuple(ids[i:i+n])\n",
        "        if ng in seen: reps += 1\n",
        "        seen.add(ng)\n",
        "    return reps / max(1, len(ids)-n+1)\n",
        "\n",
        "def _quality_score(text, ids, target_len=40):\n",
        "    uniq = len(set(ids)) / max(1, len(ids))\n",
        "    rep3 = _repetition_score(ids, n=3)\n",
        "    punct = 1.0 if any(ch in text for ch in \".!?\") else 0.0\n",
        "    len_pen = abs(len(ids) - target_len) / target_len\n",
        "    return (1.2*uniq) + (0.2*punct) - (0.8*rep3) - (0.4*len_pen)\n",
        "\n",
        "# ---------- Generation (fix cut-offs) ----------\n",
        "def _ends_with(seq, suffix):\n",
        "    L = len(suffix); return L>0 and len(seq)>=L and seq[-L:] == suffix\n",
        "\n",
        "def _first_stop_index(gen_ids, stop_seqs):\n",
        "    N = len(gen_ids); cut = N\n",
        "    for i in range(N):\n",
        "        for s in stop_seqs:\n",
        "            L = len(s)\n",
        "            if L and i+L <= N and gen_ids[i:i+L] == s:\n",
        "                cut = min(cut, i)\n",
        "    return cut\n",
        "\n",
        "def _block_repeated_ngrams(logits_row, full_ids, n=3, window=256):\n",
        "    ctx = full_ids[-window:]\n",
        "    if len(ctx) < n-1: return\n",
        "    prev = tuple(ctx[-(n-1):])\n",
        "    forbid = set()\n",
        "    for i in range(len(ctx)-(n-1)):\n",
        "        if tuple(ctx[i:i+n-1]) == prev:\n",
        "            forbid.add(ctx[i+n-1])\n",
        "    for t in forbid:\n",
        "        logits_row[t] = -float(\"inf\")\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_from_seed(seed,\n",
        "                       temperature=0.65, top_p=0.88, top_k=80,\n",
        "                       max_new_tokens=96, min_tokens_before_stop=18,\n",
        "                       repetition_penalty=1.28, penalty_ctx=220,\n",
        "                       no_repeat_ngram=3,\n",
        "                       # stop only on <eot>/<eos>\n",
        "                       eot_bias_after=None, eot_bias=1.2,\n",
        "                       best_of=2):\n",
        "    # Determine when to start nudging <eot>\n",
        "    if eot_bias_after is None:\n",
        "        eot_bias_after = max(28, min(56, ctx_len // 2 + 12))\n",
        "\n",
        "    STOP_TAGS = [\"<eot>\", \"<eos>\"]              # <-- only these cause stop/trim\n",
        "    STOP_SEQS = [sp.encode(s, out_type=int) for s in STOP_TAGS]\n",
        "    FIRST_TOKENS = {seq[0] for seq in STOP_SEQS if len(seq) > 0}\n",
        "    # Always forbid generating control tags in the middle of the reply\n",
        "    FORBID_TOKENS = [ID[\"<A>\"], ID[\"<B>\"], ID[\"<bos>\"]]\n",
        "\n",
        "    def sample_once():\n",
        "        x = torch.tensor(sp.encode(seed, out_type=int), dtype=torch.long, device=device)[None, ...]\n",
        "        start_len = x.size(1)\n",
        "        for step in range(max_new_tokens):\n",
        "            if x.size(1) > ctx_len: x = x[:, -ctx_len:]\n",
        "\n",
        "            with torch.amp.autocast(\"cuda\", dtype=amp_dtype, enabled=(device==\"cuda\")):\n",
        "                logits = model(x)[:, -1, :]\n",
        "\n",
        "            # temperature\n",
        "            logits = logits / max(1e-8, temperature)\n",
        "\n",
        "            # repetition penalty\n",
        "            if repetition_penalty and penalty_ctx > 0:\n",
        "                recent = x[0, max(0, x.size(1)-penalty_ctx):].tolist()\n",
        "                for t in set(recent):\n",
        "                    if logits[0, t] > 0: logits[0, t] /= repetition_penalty\n",
        "                    else:                logits[0, t] *= repetition_penalty\n",
        "\n",
        "            # n-gram blocking\n",
        "            if no_repeat_ngram and no_repeat_ngram >= 2:\n",
        "                _block_repeated_ngrams(logits[0], x[0].tolist(), n=no_repeat_ngram, window=ctx_len)\n",
        "\n",
        "            # gently encourage finishing after some length\n",
        "            gen_len = x.size(1) - start_len\n",
        "            if gen_len >= eot_bias_after:\n",
        "                logits[0, ID[\"<eot>\"]] += eot_bias\n",
        "\n",
        "            # NEVER allow control tags mid-reply\n",
        "            for t in FORBID_TOKENS:\n",
        "                logits[0, t] = -float(\"inf\")\n",
        "\n",
        "            # before min length, also forbid starting any stop sequence\n",
        "            if gen_len < min_tokens_before_stop:\n",
        "                for t0 in FIRST_TOKENS:\n",
        "                    logits[0, t0] = -float(\"inf\")\n",
        "\n",
        "            # probs\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "            # top-k\n",
        "            if top_k and top_k > 0:\n",
        "                topk_vals, topk_idx = torch.topk(probs, k=min(top_k, probs.size(-1)))\n",
        "                mask = torch.ones_like(probs, dtype=torch.bool); mask.scatter_(1, topk_idx, False)\n",
        "                probs[mask] = 0\n",
        "            # top-p\n",
        "            sorted_probs, sorted_idx = torch.sort(probs, descending=True)\n",
        "            cum = torch.cumsum(sorted_probs, dim=-1)\n",
        "            mask = cum > top_p; mask[..., 0] = False\n",
        "            sorted_probs[mask] = 0\n",
        "            sorted_probs /= sorted_probs.sum(dim=-1, keepdim=True)\n",
        "\n",
        "            next_id = sorted_idx.gather(-1, torch.multinomial(sorted_probs, 1))\n",
        "            x = torch.cat([x, next_id], dim=1)\n",
        "\n",
        "            # early stop after min length if tail matches stop\n",
        "            gen_ids = x[0, start_len:].tolist()\n",
        "            if gen_len + 1 >= min_tokens_before_stop:\n",
        "                if any(_ends_with(gen_ids, s) for s in STOP_SEQS):\n",
        "                    break\n",
        "\n",
        "        # trim only on <eot>/<eos>\n",
        "        gen_ids = x[0, start_len:].tolist()\n",
        "        cut_at = _first_stop_index(gen_ids, STOP_SEQS)\n",
        "        gen_ids = gen_ids[:cut_at]\n",
        "        text = sp.decode(gen_ids)\n",
        "        return detok_cleanup(text), gen_ids\n",
        "\n",
        "    if best_of <= 1:\n",
        "        text, _ids = sample_once()\n",
        "        return text\n",
        "    cands = [sample_once() for _ in range(best_of)]\n",
        "    scored = [( _quality_score(t, ids), t) for (t, ids) in cands]\n",
        "    scored.sort(key=lambda x: x[0], reverse=True)\n",
        "    return scored[0][1]\n",
        "\n",
        "# ---- REPL ----\n",
        "print(\"\\nChat ready. Type 'reset' to clear history, 'exit' to quit.\")\n",
        "print(\"Runtime controls: /temp 0.65   /topp 0.88   /topk 80   /norep 3   /eot 1.2 40   /bestof 2\\n\")\n",
        "\n",
        "history = []\n",
        "params = dict(temperature=0.65, top_p=0.88, top_k=80,\n",
        "              repetition_penalty=1.28, penalty_ctx=220,\n",
        "              no_repeat_ngram=3, eot_bias=1.2, eot_bias_after=None,\n",
        "              best_of=2)\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        user = input(\"You: \").strip()\n",
        "    except EOFError:\n",
        "        break\n",
        "    if not user: continue\n",
        "    low = user.lower()\n",
        "    if low in {\"exit\",\"quit\"}: break\n",
        "    if low==\"reset\":\n",
        "        history.clear(); print(\"Bot: (history cleared)\\n\"); continue\n",
        "\n",
        "    # live knobs\n",
        "    try:\n",
        "        if low.startswith(\"/temp \"):   params[\"temperature\"] = float(user.split()[1]); print(\"(ok)\"); continue\n",
        "        if low.startswith(\"/topp \"):   params[\"top_p\"] = float(user.split()[1]); print(\"(ok)\"); continue\n",
        "        if low.startswith(\"/topk \"):   params[\"top_k\"] = int(user.split()[1]); print(\"(ok)\"); continue\n",
        "        if low.startswith(\"/norep \"):  params[\"no_repeat_ngram\"] = int(user.split()[1]); print(\"(ok)\"); continue\n",
        "        if low.startswith(\"/eot \"):\n",
        "            _, b, a = user.split(); params[\"eot_bias\"]=float(b); params[\"eot_bias_after\"]=int(a); print(\"(ok)\"); continue\n",
        "        if low.startswith(\"/bestof \"): params[\"best_of\"] = max(1, int(user.split()[1])); print(\"(ok)\"); continue\n",
        "    except Exception:\n",
        "        print(\"(bad value)\"); continue\n",
        "\n",
        "    seed = build_seed_from_history(history.copy(), user)\n",
        "    bot  = generate_from_seed(seed, **params)\n",
        "    print(f\"Bot: {bot}\\n\")\n",
        "    history.append((user, bot))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzbbJXaW5N1w",
        "outputId": "f48fe999-f616-4c25-8064-bd64e7abdf9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n",
            "Loaded tokenizer: /content/spm_chat_8k.model\n",
            "Special IDs: {'<bos>': 4, '<eos>': 5, '<A>': 6, '<B>': 7, '<eot>': 8}\n",
            "Loaded model. Params=4.22M | Context len=64\n",
            "\n",
            "Chat ready. Type 'reset' to clear history, 'exit' to quit.\n",
            "Runtime controls: /temp 0.65   /topp 0.88   /topk 80   /norep 3   /eot 1.2 40   /bestof 2\n",
            "\n",
            "You: do you like fish?\n",
            "Bot: i am doing good, how are you today?? just got back from a lot of the weather.\n",
            "\n",
            "You: And how about the fish?\n",
            "Bot: I'm from this evening. Do you like to get on the best now? I need a little time for the last week, but not have been on them and just married.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}