{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM3IRoIwTd8Ty9K8UsUUtNn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GaborVxxx/tinygpt/blob/main/TinyGTP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbxiiKFQcg0l",
        "outputId": "d85fe492-92ff-42d5-8aa7-96a87714b597"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['conversation'],\n",
            "        num_rows: 13118\n",
            "    })\n",
            "})\n",
            "Columns (train): ['conversation']\n",
            "Preview:\n",
            " <bos>\n",
            "Person A: We can go to the cinema or say at home watching TV , what's it to be ?\n",
            "Person B: As far as I'm concerned , staying at home is more comfortable than going to the movies .\n",
            "Person A: Thanks , dear . I feel so tired after a whole day's work .\n",
            "<eos>\n",
            "-rw-r--r-- 1 root root 6.5M Sep 13 16:13 /content/dailydialog_corpus.txt\n",
            "<bos>\n",
            "Person A: We can go to the cinema or say at home watching TV , what's it to be ?\n",
            "Person B: As far as I'm concerned , staying at home is more comfortable than going to the movies .\n",
            "Person A: Thanks , dear . I feel so tired after a whole day's work .\n",
            "<eos>\n",
            "Vocab size: 4096  (pad,unk,bos,eos) = (0, 1, 2, 3)\n",
            "<bos> -> 4\n",
            "<eos> -> 5\n",
            "Person A: -> 6\n",
            "Person B: -> 7\n",
            "Encoded lengths: 1824858 197675\n",
            "Batches: 14256 1544\n",
            "Total parameters: 5.03M\n",
            "step 50/3000  lr 5.00e-05  loss 7.668\n",
            "step 100/3000  lr 1.00e-04  loss 6.956\n",
            "step 150/3000  lr 1.50e-04  loss 6.023\n",
            "step 200/3000  lr 2.00e-04  loss 5.261\n",
            "step 250/3000  lr 2.50e-04  loss 4.959\n",
            "step 300/3000  lr 3.00e-04  loss 4.647\n",
            "step 350/3000  lr 3.00e-04  loss 4.401\n",
            "step 400/3000  lr 2.99e-04  loss 4.327\n",
            "step 450/3000  lr 2.98e-04  loss 4.252\n",
            "step 500/3000  lr 2.96e-04  loss 4.284\n",
            "[eval @ step 500] val_loss 4.150  (21.3s)\n",
            "✓ saved /content/tinygpt_best.pt\n",
            "step 550/3000  lr 2.94e-04  loss 4.087\n",
            "step 600/3000  lr 2.92e-04  loss 4.110\n",
            "step 650/3000  lr 2.89e-04  loss 4.041\n",
            "step 700/3000  lr 2.86e-04  loss 3.923\n",
            "step 750/3000  lr 2.82e-04  loss 4.124\n",
            "step 800/3000  lr 2.78e-04  loss 4.039\n",
            "step 850/3000  lr 2.73e-04  loss 3.884\n",
            "step 900/3000  lr 2.68e-04  loss 3.955\n",
            "step 950/3000  lr 2.63e-04  loss 3.889\n",
            "step 1000/3000  lr 2.58e-04  loss 3.828\n",
            "[eval @ step 1000] val_loss 3.808  (21.7s)\n",
            "✓ saved /content/tinygpt_best.pt\n",
            "step 1050/3000  lr 2.52e-04  loss 3.891\n",
            "step 1100/3000  lr 2.46e-04  loss 3.794\n",
            "step 1150/3000  lr 2.39e-04  loss 3.745\n",
            "step 1200/3000  lr 2.32e-04  loss 3.850\n",
            "step 1250/3000  lr 2.26e-04  loss 3.750\n",
            "step 1300/3000  lr 2.18e-04  loss 3.806\n",
            "step 1350/3000  lr 2.11e-04  loss 3.775\n",
            "step 1400/3000  lr 2.04e-04  loss 3.751\n",
            "step 1450/3000  lr 1.96e-04  loss 3.737\n",
            "step 1500/3000  lr 1.88e-04  loss 3.657\n",
            "[eval @ step 1500] val_loss 3.613  (21.9s)\n",
            "✓ saved /content/tinygpt_best.pt\n",
            "step 1550/3000  lr 1.81e-04  loss 3.683\n",
            "step 1600/3000  lr 1.73e-04  loss 3.568\n",
            "step 1650/3000  lr 1.65e-04  loss 3.710\n",
            "step 1700/3000  lr 1.57e-04  loss 3.635\n",
            "step 1750/3000  lr 1.49e-04  loss 3.518\n",
            "step 1800/3000  lr 1.42e-04  loss 3.593\n",
            "step 1850/3000  lr 1.34e-04  loss 3.441\n",
            "step 1900/3000  lr 1.26e-04  loss 3.577\n",
            "step 1950/3000  lr 1.19e-04  loss 3.563\n",
            "step 2000/3000  lr 1.12e-04  loss 3.661\n",
            "[eval @ step 2000] val_loss 3.511  (22.2s)\n",
            "✓ saved /content/tinygpt_best.pt\n",
            "step 2050/3000  lr 1.04e-04  loss 3.578\n",
            "step 2100/3000  lr 9.75e-05  loss 3.468\n",
            "step 2150/3000  lr 9.08e-05  loss 3.421\n",
            "step 2200/3000  lr 8.44e-05  loss 3.512\n",
            "step 2250/3000  lr 7.82e-05  loss 3.595\n",
            "step 2300/3000  lr 7.24e-05  loss 3.481\n",
            "step 2350/3000  lr 6.68e-05  loss 3.468\n",
            "step 2400/3000  lr 6.16e-05  loss 3.430\n",
            "step 2450/3000  lr 5.67e-05  loss 3.466\n",
            "step 2500/3000  lr 5.22e-05  loss 3.642\n",
            "[eval @ step 2500] val_loss 3.476  (21.6s)\n",
            "✓ saved /content/tinygpt_best.pt\n",
            "step 2550/3000  lr 4.81e-05  loss 3.622\n",
            "step 2600/3000  lr 4.44e-05  loss 3.544\n",
            "step 2650/3000  lr 4.10e-05  loss 3.531\n",
            "step 2700/3000  lr 3.81e-05  loss 3.402\n",
            "step 2750/3000  lr 3.57e-05  loss 3.426\n",
            "step 2800/3000  lr 3.36e-05  loss 3.476\n",
            "step 2850/3000  lr 3.21e-05  loss 3.559\n",
            "step 2900/3000  lr 3.09e-05  loss 3.390\n",
            "step 2950/3000  lr 3.02e-05  loss 3.489\n",
            "step 3000/3000  lr 3.00e-05  loss 3.542\n",
            "[eval @ step 3000] val_loss 3.461  (21.8s)\n",
            "✓ saved /content/tinygpt_best.pt\n",
            "Loaded best checkpoint.\n",
            "Person A: Hi, how are you?\n",
            "Person B: Here is the right. Would you like to walk?\n",
            "----\n",
            "Person A: What's your favorite movie?\n",
            "Person B: I still really think of the moment.\n"
          ]
        }
      ],
      "source": [
        "# ================================\n",
        "# 0) Setup (installs + imports)\n",
        "# ================================\n",
        "!pip -q install datasets sentencepiece\n",
        "\n",
        "import os, math, time, re, random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from datasets import load_dataset\n",
        "import sentencepiece as spm\n",
        "\n",
        "# ---- Hyperparams / knobs ----\n",
        "block_size   = 128     # max sequence length\n",
        "batch_size   = 64      # reduce to 32/16 if OOM\n",
        "base_lr      = 3e-4\n",
        "warmup_steps = 300\n",
        "max_steps    = 3000    # bump to 20_000+ for better quality\n",
        "eval_every   = 500\n",
        "\n",
        "ROLE_A = \"Person A:\"\n",
        "ROLE_B = \"Person B:\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# Optional speedups\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "if hasattr(torch, \"set_float32_matmul_precision\"):\n",
        "    torch.set_float32_matmul_precision(\"high\")\n",
        "\n",
        "# ================================\n",
        "# 1) Load DailyDialog & format as Person A:/Person B:\n",
        "# ================================\n",
        "raw = load_dataset(\"elricwan/dailydialog\")  # public parquet mirror\n",
        "print(raw)\n",
        "\n",
        "if \"validation\" in raw:\n",
        "    ds = {\"train\": raw[\"train\"], \"validation\": raw[\"validation\"]}\n",
        "else:\n",
        "    splits = raw[\"train\"].train_test_split(test_size=0.1, seed=1337)\n",
        "    ds = {\"train\": splits[\"train\"], \"validation\": splits[\"test\"]}\n",
        "\n",
        "print(\"Columns (train):\", ds[\"train\"].column_names)\n",
        "\n",
        "def row_to_dialog_list(row):\n",
        "    # try common field names; else first column\n",
        "    for key in (\"dialog\",\"dialogs\",\"utterances\",\"texts\",\"conversation\",\"conversations\",\"data\"):\n",
        "        if key in row:\n",
        "            val = row[key]; break\n",
        "    else:\n",
        "        k0 = next(iter(row.keys())); val = row[k0]\n",
        "    # normalize to list[str]\n",
        "    if isinstance(val, list):\n",
        "        if val and isinstance(val[0], str): return val\n",
        "        if val and isinstance(val[0], dict):\n",
        "            for kk in (\"utterance\",\"text\",\"content\"):\n",
        "                if kk in val[0]: return [u.get(kk, \"\") for u in val]\n",
        "            return [str(u) for u in val]\n",
        "    return [str(val)]\n",
        "\n",
        "def strip_role_prefix(s: str) -> str:\n",
        "    s = s.strip()\n",
        "    # remove <A>, <B>, \"Person A:\", \"Person B:\", \"A:\", \"B:\" at start (case-insensitive)\n",
        "    s = re.sub(r'^(?:<A>|<B>)\\s*', '', s)\n",
        "    s = re.sub(r'^(?:Person\\s+[AB]:|[AB]:)\\s*', '', s, flags=re.IGNORECASE)\n",
        "    return s\n",
        "\n",
        "def dialog_to_text_row(row):\n",
        "    sents = row_to_dialog_list(row)\n",
        "    lines = []\n",
        "    for i, s in enumerate(sents):\n",
        "        role = ROLE_A if (i % 2 == 0) else ROLE_B\n",
        "        lines.append(f\"{role} {strip_role_prefix(s)}\")\n",
        "    return \"<bos>\\n\" + \"\\n\".join(lines) + \"\\n<eos>\"\n",
        "\n",
        "train_texts = [dialog_to_text_row(r) for r in ds[\"train\"]]\n",
        "val_texts   = [dialog_to_text_row(r) for r in ds[\"validation\"]]\n",
        "\n",
        "print(\"Preview:\\n\", train_texts[0][:500])\n",
        "\n",
        "# ================================\n",
        "# 2) Train SentencePiece tokenizer (BPE, 4k vocab) with role tokens\n",
        "# ================================\n",
        "corpus_path = \"/content/dailydialog_corpus.txt\"\n",
        "with open(corpus_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    for t in train_texts:\n",
        "        f.write(t + \"\\n\")\n",
        "\n",
        "!ls -lh /content/dailydialog_corpus.txt\n",
        "!head -n 5 /content/dailydialog_corpus.txt\n",
        "\n",
        "# retrain tokenizer (clean slate)\n",
        "!rm -f /content/spm_dd_4k.model /content/spm_dd_4k.vocab\n",
        "\n",
        "spm.SentencePieceTrainer.Train(\n",
        "    input=corpus_path,\n",
        "    model_prefix=\"/content/spm_dd_4k\",\n",
        "    vocab_size=4096,\n",
        "    model_type=\"bpe\",\n",
        "    character_coverage=1.0,\n",
        "    pad_id=0, unk_id=1, bos_id=2, eos_id=3,\n",
        "    user_defined_symbols=[\"<bos>\", \"<eos>\", \"Person A:\", \"Person B:\"]\n",
        ")\n",
        "\n",
        "sp = spm.SentencePieceProcessor(model_file=\"/content/spm_dd_4k.model\")\n",
        "vocab_size = sp.vocab_size()\n",
        "pad_id, unk_id, bos_id, eos_id = 0, 1, 2, 3\n",
        "print(\"Vocab size:\", vocab_size, \" (pad,unk,bos,eos) =\", (pad_id,unk_id,bos_id,eos_id))\n",
        "for tok in [\"<bos>\", \"<eos>\", \"Person A:\", \"Person B:\"]:\n",
        "    print(tok, \"->\", sp.piece_to_id(tok))\n",
        "\n",
        "# ================================\n",
        "# 3) Encode dataset → LM chunks\n",
        "# ================================\n",
        "def encode_texts(texts):\n",
        "    # We already put textual <bos>/<eos> into strings; just encode & flatten\n",
        "    return np.array([tid for t in texts for tid in sp.encode(t, out_type=int)], dtype=np.int32)\n",
        "\n",
        "train_ids = encode_texts(train_texts)\n",
        "val_ids   = encode_texts(val_texts)\n",
        "print(\"Encoded lengths:\", len(train_ids), len(val_ids))\n",
        "\n",
        "class LMChunkDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, ids, block_size):\n",
        "        L = (len(ids) - 1) // block_size\n",
        "        self.input  = torch.tensor(ids[:L*block_size],    dtype=torch.long).view(L, block_size)\n",
        "        self.target = torch.tensor(ids[1:L*block_size+1], dtype=torch.long).view(L, block_size)\n",
        "    def __len__(self): return self.input.size(0)\n",
        "    def __getitem__(self, idx): return self.input[idx], self.target[idx]\n",
        "\n",
        "train_ds = LMChunkDataset(train_ids, block_size)\n",
        "val_ds   = LMChunkDataset(val_ids, block_size)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True,  drop_last=True)\n",
        "val_loader   = torch.utils.data.DataLoader(val_ds,   batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "print(\"Batches:\", len(train_ds), len(val_ds))\n",
        "\n",
        "# ================================\n",
        "# 4) Define a tiny GPT (~5M params)\n",
        "# ================================\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, d_model=256, n_heads=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0\n",
        "        self.n_heads = n_heads\n",
        "        self.d_head  = d_model // n_heads\n",
        "        self.qkv   = nn.Linear(d_model, 3*d_model, bias=False)\n",
        "        self.proj  = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.attn_drop  = nn.Dropout(dropout)\n",
        "        self.resid_drop = nn.Dropout(dropout)\n",
        "        self.register_buffer(\"mask\", None, persistent=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        if (self.mask is None) or (self.mask.size(-1) < T):\n",
        "            self.mask = torch.tril(torch.ones(T, T, device=x.device)).view(1,1,T,T)\n",
        "        qkv = self.qkv(x)                            # (B,T,3C)\n",
        "        q, k, v = qkv.split(C, dim=2)\n",
        "        def split_heads(t): return t.view(B, T, self.n_heads, self.d_head).transpose(1,2)\n",
        "        q, k, v = map(split_heads, (q, k, v))        # (B,H,T,Dh)\n",
        "        att = (q @ k.transpose(-2, -1)) / (self.d_head ** 0.5)\n",
        "        att = att.masked_fill(self.mask[:, :, :T, :T] == 0, float(\"-inf\"))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attn_drop(att)\n",
        "        y = att @ v                                  # (B,H,T,Dh)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.resid_drop(self.proj(y))\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, d_model=256, d_ff=1024, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "    def forward(self, x):\n",
        "        return self.drop(self.fc2(self.act(self.fc1(x))))\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, d_model=256, n_heads=4, d_ff=1024, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.attn = CausalSelfAttention(d_model, n_heads, dropout)\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "        self.mlp  = MLP(d_model, d_ff, dropout)\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class TinyGPT(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=256, n_heads=4, d_ff=1024, n_layers=5, max_seq_len=128, dropout=0.1, pad_id=0):\n",
        "        super().__init__()\n",
        "        self.token_embed = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n",
        "        self.pos_embed   = nn.Embedding(max_seq_len, d_model)\n",
        "        self.drop  = nn.Dropout(dropout)\n",
        "        self.blocks = nn.ModuleList([Block(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)])\n",
        "        self.ln_f  = nn.LayerNorm(d_model)\n",
        "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
        "        # weight tying\n",
        "        self.lm_head.weight = self.token_embed.weight\n",
        "        self.apply(self._init)\n",
        "\n",
        "    @staticmethod\n",
        "    def _init(m):\n",
        "        if isinstance(m, (nn.Linear, nn.Embedding)):\n",
        "            nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
        "        if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "            nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, idx):\n",
        "        B, T = idx.size()\n",
        "        pos = torch.arange(0, T, device=idx.device).unsqueeze(0)\n",
        "        x = self.token_embed(idx) + self.pos_embed(pos)\n",
        "        x = self.drop(x)\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "        x = self.ln_f(x)\n",
        "        return self.lm_head(x)\n",
        "\n",
        "model = TinyGPT(vocab_size=vocab_size, d_model=256, n_heads=4, d_ff=1024,\n",
        "                n_layers=5, max_seq_len=block_size, dropout=0.1, pad_id=pad_id).to(device)\n",
        "\n",
        "n_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total parameters: {n_params/1e6:.2f}M\")\n",
        "\n",
        "# ================================\n",
        "# 5) Train (AdamW, warmup + cosine, AMP new API, grad clip)\n",
        "# ================================\n",
        "def get_lr(step, warmup, max_steps, base_lr):\n",
        "    if step < warmup:\n",
        "        return base_lr * step / max(1, warmup)\n",
        "    progress = (step - warmup) / max(1, (max_steps - warmup))\n",
        "    return 0.1*base_lr + 0.9*base_lr * 0.5 * (1 + math.cos(math.pi * progress))\n",
        "\n",
        "# (Optional) better weight decay: don't decay LayerNorm/bias\n",
        "decay, no_decay = [], []\n",
        "for n,p in model.named_parameters():\n",
        "    if not p.requires_grad: continue\n",
        "    if n.endswith(\"bias\") or \"ln\" in n.lower() or \"layernorm\" in n.lower():\n",
        "        no_decay.append(p)\n",
        "    else:\n",
        "        decay.append(p)\n",
        "optimizer = torch.optim.AdamW(\n",
        "    [{\"params\": decay,    \"weight_decay\": 0.1},\n",
        "     {\"params\": no_decay, \"weight_decay\": 0.0}],\n",
        "    lr=base_lr, betas=(0.9, 0.95)\n",
        ")\n",
        "\n",
        "scaler = torch.amp.GradScaler(\"cuda\", enabled=(device==\"cuda\"))\n",
        "\n",
        "def run_eval():\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in val_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            with torch.amp.autocast(\"cuda\", enabled=(device==\"cuda\")):\n",
        "                logits = model(xb)\n",
        "                loss = F.cross_entropy(logits.view(-1, vocab_size), yb.view(-1), ignore_index=pad_id)\n",
        "            losses.append(loss.item())\n",
        "    model.train()\n",
        "    return sum(losses)/len(losses) if losses else float(\"nan\")\n",
        "\n",
        "model.train()\n",
        "global_step = 0\n",
        "best_val = float(\"inf\")\n",
        "t0 = time.time()\n",
        "\n",
        "for epoch in range(999999):\n",
        "    for xb, yb in train_loader:\n",
        "        global_step += 1\n",
        "        lr = get_lr(global_step, warmup_steps, max_steps, base_lr)\n",
        "        for pg in optimizer.param_groups: pg[\"lr\"] = lr\n",
        "\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with torch.amp.autocast(\"cuda\", enabled=(device==\"cuda\")):\n",
        "            logits = model(xb)\n",
        "            loss = F.cross_entropy(logits.view(-1, vocab_size), yb.view(-1), ignore_index=pad_id)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        if global_step % 50 == 0:\n",
        "            print(f\"step {global_step}/{max_steps}  lr {lr:.2e}  loss {loss.item():.3f}\")\n",
        "\n",
        "        if global_step % eval_every == 0:\n",
        "            val_loss = run_eval()\n",
        "            dt = time.time() - t0\n",
        "            print(f\"[eval @ step {global_step}] val_loss {val_loss:.3f}  ({dt:.1f}s)\")\n",
        "            t0 = time.time()\n",
        "            if val_loss < best_val:\n",
        "                best_val = val_loss\n",
        "                torch.save({\"model\": model.state_dict(),\n",
        "                            \"config\": {\"vocab_size\": vocab_size, \"block_size\": block_size}},\n",
        "                           \"/content/tinygpt_best.pt\")\n",
        "                print(\"✓ saved /content/tinygpt_best.pt\")\n",
        "\n",
        "        if global_step >= max_steps:\n",
        "            break\n",
        "    if global_step >= max_steps:\n",
        "        break\n",
        "\n",
        "# ================================\n",
        "# 6) Sampling (nucleus + temperature) with role tags\n",
        "# ================================\n",
        "@torch.no_grad()\n",
        "def sample(\n",
        "    model, sp, prompt,\n",
        "    max_new_tokens=120,\n",
        "    temperature=0.85,\n",
        "    top_p=0.92,\n",
        "    min_tokens_before_stop=20,\n",
        "    repetition_penalty=1.12,\n",
        "    penalty_ctx=80,\n",
        "    include_prompt=False\n",
        "):\n",
        "    model.eval()\n",
        "\n",
        "    # seed as a dialogue turn\n",
        "    seed = \"<bos>\\nPerson A: \" + prompt.strip() + \"\\nPerson B: \"\n",
        "    x = torch.tensor(sp.encode(seed, out_type=int), dtype=torch.long, device=device)[None, ...]\n",
        "    start_len = x.size(1)\n",
        "\n",
        "    # prefetch special piece IDs (guaranteed to exist: we added them as user_defined_symbols)\n",
        "    id_A = sp.piece_to_id(\"Person A:\")\n",
        "    id_B = sp.piece_to_id(\"Person B:\")\n",
        "    # eos is handled via eos_id variable from earlier\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        if x.size(1) > block_size:\n",
        "            x = x[:, -block_size:]\n",
        "\n",
        "        with torch.amp.autocast(\"cuda\", enabled=(device == \"cuda\")):\n",
        "            logits = model(x)[:, -1, :]\n",
        "\n",
        "        # temperature\n",
        "        logits = logits / max(1e-8, temperature)\n",
        "\n",
        "        # (optional) repetition penalty over recent context\n",
        "        if repetition_penalty and penalty_ctx > 0:\n",
        "            recent = x[0, max(0, x.size(1) - penalty_ctx):].tolist()\n",
        "            for t in set(recent):\n",
        "                logits[0, t] /= repetition_penalty\n",
        "\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "        # nucleus (top-p) filtering\n",
        "        sorted_probs, sorted_idx = torch.sort(probs, descending=True)\n",
        "        cum = torch.cumsum(sorted_probs, dim=-1)\n",
        "        mask = cum > top_p\n",
        "        mask[..., 0] = False\n",
        "        sorted_probs[mask] = 0\n",
        "        sorted_probs = sorted_probs / sorted_probs.sum(dim=-1, keepdim=True)\n",
        "\n",
        "        next_id = sorted_idx.gather(-1, torch.multinomial(sorted_probs, 1))  # (1,1)\n",
        "        token = next_id.item()\n",
        "\n",
        "        x = torch.cat([x, next_id], dim=1)\n",
        "\n",
        "        # stop if we see EOS or a new turn tag, but only after a little length\n",
        "        gen_len = x.size(1) - start_len\n",
        "        if gen_len >= min_tokens_before_stop and (token in (eos_id, id_A, id_B)):\n",
        "            break\n",
        "\n",
        "    # decode only what was generated for Person B\n",
        "    new_tokens = x[0, start_len:].tolist()\n",
        "    text = sp.decode(new_tokens)\n",
        "\n",
        "    # extra safety: trim if string contains any turn tags or <eos>\n",
        "    for stop in [\"<eos>\", \"Person A:\", \"Person B:\"]:\n",
        "        i = text.find(stop)\n",
        "        if i != -1:\n",
        "            text = text[:i]\n",
        "            break\n",
        "\n",
        "    # light detok cleanup for readability (DailyDialog has spaced punctuation)\n",
        "    text = (\n",
        "        text.replace(\" ,\", \",\")\n",
        "            .replace(\" .\", \".\")\n",
        "            .replace(\" !\", \"!\")\n",
        "            .replace(\" ?\", \"?\")\n",
        "            .replace(\" ’ \", \"’\")\n",
        "            .replace(\" ' s\", \"’s\")\n",
        "            .replace(\" ' m\", \"’m\")\n",
        "            .replace(\" ' ve\", \"’ve\")\n",
        "            .replace(\" ' re\", \"’re\")\n",
        "    )\n",
        "    reply = text.strip()\n",
        "    if include_prompt:\n",
        "        return f\"Person A: {prompt.strip()}\\nPerson B: {reply}\"\n",
        "    return reply\n",
        "\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 7) Load best (if any) and chat\n",
        "# ================================\n",
        "ckpt_path = \"/content/tinygpt_best.pt\"\n",
        "if os.path.exists(ckpt_path):\n",
        "    sd = torch.load(ckpt_path, map_location=device)\n",
        "    model.load_state_dict(sd[\"model\"])\n",
        "    print(\"Loaded best checkpoint.\")\n",
        "\n",
        "print(sample(model, sp, \"Hi, how are you?\", max_new_tokens=80, include_prompt=True))\n",
        "print(\"----\")\n",
        "print(sample(model, sp, \"What's your favorite movie?\", max_new_tokens=80, include_prompt=True))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# Self-contained history-aware console chat\n",
        "# ============================\n",
        "import os, torch, sentencepiece as spm\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "sp_model_path = \"/content/spm_dd_4k.model\"\n",
        "ckpt_path     = \"/content/tinygpt_best.pt\"\n",
        "assert os.path.exists(sp_model_path), f\"Missing tokenizer at {sp_model_path}\"\n",
        "assert os.path.exists(ckpt_path), f\"Missing checkpoint at {ckpt_path}\"\n",
        "\n",
        "# ---- Load tokenizer ----\n",
        "sp = spm.SentencePieceProcessor(model_file=sp_model_path)\n",
        "pad_id, unk_id, bos_id, eos_id = 0, 1, 2, 3\n",
        "\n",
        "\n",
        "# ---- Load checkpoint & build model ----\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "ckpt = torch.load(ckpt_path, map_location=device)\n",
        "cfg  = ckpt.get(\"config\", {})\n",
        "block_size = cfg.get(\"block_size\", 128)\n",
        "vocab_size = cfg.get(\"vocab_size\", sp.vocab_size())\n",
        "\n",
        "model = TinyGPT(vocab_size=vocab_size, d_model=256, n_heads=4, d_ff=1024,\n",
        "                n_layers=5, max_seq_len=block_size, dropout=0.1, pad_id=0).to(device)\n",
        "model.load_state_dict(ckpt[\"model\"]); model.eval()\n",
        "print(\"Loaded model & tokenizer. Context len =\", block_size)\n",
        "\n",
        "# ---- Helpers ----\n",
        "def detok_cleanup(txt: str) -> str:\n",
        "    return (txt.replace(\" ,\", \",\").replace(\" .\", \".\").replace(\" !\", \"!\")\n",
        "              .replace(\" ?\", \"?\").replace(\" ’ \", \"’\")\n",
        "              .replace(\" ' s\",\"’s\").replace(\" ' m\",\"’m\")\n",
        "              .replace(\" ' ve\",\"’ve\").replace(\" ' re\",\"’re\")).strip()\n",
        "\n",
        "# Build seed from recent history (keeps context within block_size)\n",
        "MAX_CTX_TOKENS = block_size\n",
        "RESERVED_GEN_TOKENS = 64  # leave room for the new reply\n",
        "\n",
        "def build_seed_from_history(history, user_msg):\n",
        "    def convo(turns, last_user):\n",
        "        lines = []\n",
        "        for u, b in turns:\n",
        "            if u: lines.append(f\"Person A: {u.strip()}\")\n",
        "            if b: lines.append(f\"Person B: {b.strip()}\")\n",
        "        lines.append(f\"Person A: {last_user.strip()}\")\n",
        "        return \"<bos>\\n\" + \"\\n\".join(lines) + \"\\nPerson B: \"\n",
        "    seed = convo(history, user_msg); ids = sp.encode(seed, out_type=int)\n",
        "    while len(ids) > (MAX_CTX_TOKENS - RESERVED_GEN_TOKENS) and history:\n",
        "        history.pop(0)  # drop oldest turn\n",
        "        seed = convo(history, user_msg); ids = sp.encode(seed, out_type=int)\n",
        "    return seed\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_from_seed(seed,\n",
        "                       temperature=0.80, top_p=0.95,\n",
        "                       max_new_tokens=120, min_tokens_before_stop=24,\n",
        "                       repetition_penalty=1.15, penalty_ctx=80):\n",
        "    # Build stop sequences as token *lists* (handles multi-token cases)\n",
        "    STOP_STRINGS = [\"<eos>\", \"<bos>\", \"Person A:\", \"Person B:\"]\n",
        "    STOP_SEQS = [sp.encode(s, out_type=int) for s in STOP_STRINGS]\n",
        "    FIRST_TOKENS = {seq[0] for seq in STOP_SEQS if len(seq) > 0}\n",
        "\n",
        "    def ends_with(seq, suffix):\n",
        "        L = len(suffix)\n",
        "        return L > 0 and len(seq) >= L and seq[-L:] == suffix\n",
        "\n",
        "    def find_first_stop(gen_ids):\n",
        "        # cut at the earliest occurrence of any STOP_SEQ\n",
        "        N = len(gen_ids)\n",
        "        cut = N\n",
        "        for i in range(N):\n",
        "            for s in STOP_SEQS:\n",
        "                L = len(s)\n",
        "                if L and i+L <= N and gen_ids[i:i+L] == s:\n",
        "                    cut = min(cut, i)\n",
        "        return cut\n",
        "\n",
        "    x = torch.tensor(sp.encode(seed, out_type=int), dtype=torch.long, device=device)[None, ...]\n",
        "    start_len = x.size(1)\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        if x.size(1) > block_size:\n",
        "            x = x[:, -block_size:]\n",
        "\n",
        "        with torch.amp.autocast(\"cuda\", enabled=(device==\"cuda\")):\n",
        "            logits = model(x)[:, -1, :]\n",
        "\n",
        "        # temperature\n",
        "        logits = logits / max(1e-8, temperature)\n",
        "\n",
        "        # repetition penalty on recent context\n",
        "        if repetition_penalty and penalty_ctx > 0:\n",
        "            recent = x[0, max(0, x.size(1)-penalty_ctx):].tolist()\n",
        "            for t in set(recent):\n",
        "                logits[0, t] /= repetition_penalty\n",
        "\n",
        "        # (optional) forbid starting any stop sequence before min length\n",
        "        gen_len = x.size(1) - start_len\n",
        "        if gen_len < min_tokens_before_stop:\n",
        "            for t0 in FIRST_TOKENS:\n",
        "                logits[0, t0] = -float(\"inf\")\n",
        "\n",
        "        # nucleus (top-p)\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        sorted_probs, sorted_idx = torch.sort(probs, descending=True)\n",
        "        cum = torch.cumsum(sorted_probs, dim=-1)\n",
        "        mask = cum > top_p\n",
        "        mask[..., 0] = False\n",
        "        sorted_probs[mask] = 0\n",
        "        sorted_probs /= sorted_probs.sum(dim=-1, keepdim=True)\n",
        "\n",
        "        next_id = sorted_idx.gather(-1, torch.multinomial(sorted_probs, 1))\n",
        "        x = torch.cat([x, next_id], dim=1)\n",
        "\n",
        "        # early stop if the *generated* tail matches any stop sequence\n",
        "        gen_ids = x[0, start_len:].tolist()\n",
        "        if gen_len + 1 >= min_tokens_before_stop:\n",
        "            if any(ends_with(gen_ids, s) for s in STOP_SEQS):\n",
        "                break\n",
        "\n",
        "    # Trim at the first stop *sequence* (robust against multi-token tags)\n",
        "    gen_ids = x[0, start_len:].tolist()\n",
        "    cut_at = find_first_stop(gen_ids)\n",
        "    gen_ids = gen_ids[:cut_at]\n",
        "\n",
        "    # decode & clean\n",
        "    txt = sp.decode(gen_ids)\n",
        "    txt = (txt.replace(\" ,\", \",\").replace(\" .\", \".\").replace(\" !\", \"!\")\n",
        "              .replace(\" ?\", \"?\").replace(\" ’ \", \"’\")\n",
        "              .replace(\" ' s\",\"’s\").replace(\" ' m\",\"’m\")\n",
        "              .replace(\" ' ve\",\"’ve\").replace(\" ' re\",\"’re\")\n",
        "              .replace(\" ' d\",\"’d\").replace(\" ' ll\",\"’ll\").replace(\" n't\",\"n’t\"))\n",
        "    return txt.strip()\n",
        "\n",
        "\n",
        "# ---- Console chat loop (single reply per turn, history-aware) ----\n",
        "print(\"\\nChat ready. Type 'reset' to clear history, or 'exit' to quit.\\n\")\n",
        "history = []\n",
        "while True:\n",
        "    user = input(\"You: \").strip()\n",
        "    if user.lower() in {\"exit\", \"quit\"}: break\n",
        "    if user.lower() == \"reset\":\n",
        "        history.clear(); print(\"Bot: (history cleared)\\n\"); continue\n",
        "    seed = build_seed_from_history(history.copy(), user)\n",
        "    bot = generate_from_seed(seed)\n",
        "    print(f\"Bot: {bot}\\n\")\n",
        "    history.append((user, bot))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzbbJXaW5N1w",
        "outputId": "9a2dac43-444c-4993-807f-825d646b2960"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded model & tokenizer. Context len = 128\n",
            "\n",
            "Chat ready. Type 'reset' to clear history, or 'exit' to quit.\n",
            "\n",
            "You: hello there\n",
            "Bot: I'd like to see. How are you going? Does it start? It seems that's really nice of dollars a good job.\n",
            "\n",
            "You: nice\n",
            "Bot: Wow, they're very spicy people who think we can cut it. The pildcre not be careful.\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2274848515.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m     \u001b[0muser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"exit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"quit\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"reset\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    }
  ]
}