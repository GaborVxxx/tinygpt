{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPwnuqMrjIhZqYwNkGlP4S3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GaborVxxx/tinygpt/blob/main/TinyGTP2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbxiiKFQcg0l",
        "outputId": "adf0c850-3e17-46c4-859a-1593d9e774b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['conversation'],\n",
            "        num_rows: 13118\n",
            "    })\n",
            "})\n",
            "Columns (train): ['conversation']\n",
            "Preview (formatted):\n",
            " <bos>\n",
            "<A> We can go to the cinema or say at home watching TV, what's it to be? <eot>\n",
            "<B> As far as I'm concerned, staying at home is more comfortable than going to the movies. <eot>\n",
            "<A> Thanks, dear. I feel so tired after a whole day's work. <eot>\n",
            "<eos>\n",
            "Validation — dropped: train 0, val 0\n",
            "Final counts — train 11806, val 1312\n",
            "-rw-r--r-- 1 root root 6.3M Sep 14 08:23 /content/dailydialog_corpus.txt\n",
            "<bos>\n",
            "<A> We can go to the cinema or say at home watching TV, what's it to be? <eot>\n",
            "<B> As far as I'm concerned, staying at home is more comfortable than going to the movies. <eot>\n",
            "<A> Thanks, dear. I feel so tired after a whole day's work. <eot>\n",
            "<eos>\n",
            "Vocab size: 8000  (pad,unk,bos,eos) = (0, 1, 2, 3)\n",
            "<bos> -> 4\n",
            "<eos> -> 5\n",
            "<A> -> 6\n",
            "<B> -> 7\n",
            "<eot> -> 8\n",
            "Encoded lengths: 1833130 199209\n",
            "Batches: 14321 1556\n",
            "Total parameters: 6.02M\n",
            "step 50/20000  lr 1.50e-05  loss 8.584\n",
            "step 100/20000  lr 3.00e-05  loss 8.170\n",
            "step 150/20000  lr 4.50e-05  loss 7.811\n",
            "step 200/20000  lr 6.00e-05  loss 7.380\n",
            "step 250/20000  lr 7.50e-05  loss 6.781\n",
            "step 300/20000  lr 9.00e-05  loss 6.206\n",
            "step 350/20000  lr 1.05e-04  loss 5.720\n",
            "step 400/20000  lr 1.20e-04  loss 5.292\n",
            "step 450/20000  lr 1.35e-04  loss 5.129\n",
            "step 500/20000  lr 1.50e-04  loss 4.839\n",
            "step 550/20000  lr 1.65e-04  loss 4.681\n",
            "step 600/20000  lr 1.80e-04  loss 4.399\n",
            "step 650/20000  lr 1.95e-04  loss 4.477\n",
            "step 700/20000  lr 2.10e-04  loss 4.480\n",
            "step 750/20000  lr 2.25e-04  loss 4.305\n",
            "step 800/20000  lr 2.40e-04  loss 4.252\n",
            "step 850/20000  lr 2.55e-04  loss 4.177\n",
            "step 900/20000  lr 2.70e-04  loss 4.095\n",
            "step 950/20000  lr 2.85e-04  loss 4.009\n",
            "step 1000/20000  lr 3.00e-04  loss 3.833\n",
            "[eval @ step 1000] val_loss 3.889  (48.3s)\n",
            "✓ saved /content/tinygpt_best.pt\n",
            "step 1050/20000  lr 3.00e-04  loss 4.059\n",
            "step 1100/20000  lr 3.00e-04  loss 3.957\n",
            "step 1150/20000  lr 3.00e-04  loss 3.848\n",
            "step 1200/20000  lr 3.00e-04  loss 3.895\n",
            "step 1250/20000  lr 3.00e-04  loss 3.849\n",
            "step 1300/20000  lr 3.00e-04  loss 3.659\n",
            "step 1350/20000  lr 3.00e-04  loss 3.755\n",
            "step 1400/20000  lr 3.00e-04  loss 3.717\n",
            "step 1450/20000  lr 3.00e-04  loss 3.662\n",
            "step 1500/20000  lr 3.00e-04  loss 3.595\n",
            "step 1550/20000  lr 2.99e-04  loss 3.637\n",
            "step 1600/20000  lr 2.99e-04  loss 3.763\n",
            "step 1650/20000  lr 2.99e-04  loss 3.464\n",
            "step 1700/20000  lr 2.99e-04  loss 3.569\n",
            "step 1750/20000  lr 2.99e-04  loss 3.611\n",
            "step 1800/20000  lr 2.99e-04  loss 3.526\n",
            "step 1850/20000  lr 2.99e-04  loss 3.570\n",
            "step 1900/20000  lr 2.99e-04  loss 3.701\n",
            "step 1950/20000  lr 2.98e-04  loss 3.498\n",
            "step 2000/20000  lr 2.98e-04  loss 3.535\n",
            "[eval @ step 2000] val_loss 3.436  (49.6s)\n",
            "✓ saved /content/tinygpt_best.pt\n",
            "step 2050/20000  lr 2.98e-04  loss 3.539\n",
            "step 2100/20000  lr 2.98e-04  loss 3.435\n",
            "step 2150/20000  lr 2.98e-04  loss 3.532\n",
            "step 2200/20000  lr 2.97e-04  loss 3.382\n",
            "step 2250/20000  lr 2.97e-04  loss 3.423\n",
            "step 2300/20000  lr 2.97e-04  loss 3.483\n",
            "step 2350/20000  lr 2.97e-04  loss 3.363\n",
            "step 2400/20000  lr 2.96e-04  loss 3.457\n",
            "step 2450/20000  lr 2.96e-04  loss 3.462\n",
            "step 2500/20000  lr 2.96e-04  loss 3.456\n",
            "step 2550/20000  lr 2.96e-04  loss 3.435\n",
            "step 2600/20000  lr 2.95e-04  loss 3.370\n",
            "step 2650/20000  lr 2.95e-04  loss 3.448\n",
            "step 2700/20000  lr 2.95e-04  loss 3.429\n",
            "step 2750/20000  lr 2.94e-04  loss 3.376\n",
            "step 2800/20000  lr 2.94e-04  loss 3.270\n",
            "step 2850/20000  lr 2.94e-04  loss 3.357\n",
            "step 2900/20000  lr 2.93e-04  loss 3.379\n",
            "step 2950/20000  lr 2.93e-04  loss 3.461\n",
            "step 3000/20000  lr 2.93e-04  loss 3.398\n",
            "[eval @ step 3000] val_loss 3.296  (50.6s)\n",
            "✓ saved /content/tinygpt_best.pt\n",
            "step 3050/20000  lr 2.92e-04  loss 3.368\n",
            "step 3100/20000  lr 2.92e-04  loss 3.233\n",
            "step 3150/20000  lr 2.92e-04  loss 3.285\n",
            "step 3200/20000  lr 2.91e-04  loss 3.500\n",
            "step 3250/20000  lr 2.91e-04  loss 3.258\n",
            "step 3300/20000  lr 2.90e-04  loss 3.433\n",
            "step 3350/20000  lr 2.90e-04  loss 3.264\n",
            "step 3400/20000  lr 2.90e-04  loss 3.390\n",
            "step 3450/20000  lr 2.89e-04  loss 3.385\n",
            "step 3500/20000  lr 2.89e-04  loss 3.268\n",
            "step 3550/20000  lr 2.88e-04  loss 3.214\n",
            "step 3600/20000  lr 2.88e-04  loss 3.187\n",
            "step 3650/20000  lr 2.87e-04  loss 3.270\n",
            "step 3700/20000  lr 2.87e-04  loss 3.210\n",
            "step 3750/20000  lr 2.86e-04  loss 3.286\n",
            "step 3800/20000  lr 2.86e-04  loss 3.205\n",
            "step 3850/20000  lr 2.85e-04  loss 3.230\n",
            "step 3900/20000  lr 2.85e-04  loss 3.186\n",
            "step 3950/20000  lr 2.84e-04  loss 3.315\n",
            "step 4000/20000  lr 2.84e-04  loss 3.192\n",
            "[eval @ step 4000] val_loss 3.199  (51.6s)\n",
            "✓ saved /content/tinygpt_best.pt\n",
            "step 4050/20000  lr 2.83e-04  loss 3.248\n",
            "step 4100/20000  lr 2.83e-04  loss 3.243\n",
            "step 4150/20000  lr 2.82e-04  loss 3.241\n",
            "step 4200/20000  lr 2.82e-04  loss 3.249\n",
            "step 4250/20000  lr 2.81e-04  loss 3.287\n",
            "step 4300/20000  lr 2.80e-04  loss 3.201\n",
            "step 4350/20000  lr 2.80e-04  loss 3.241\n",
            "step 4400/20000  lr 2.79e-04  loss 3.026\n",
            "step 4450/20000  lr 2.79e-04  loss 3.146\n",
            "step 4500/20000  lr 2.78e-04  loss 3.219\n",
            "step 4550/20000  lr 2.77e-04  loss 3.225\n",
            "step 4600/20000  lr 2.77e-04  loss 3.230\n",
            "step 4650/20000  lr 2.76e-04  loss 3.181\n",
            "step 4700/20000  lr 2.76e-04  loss 3.190\n",
            "step 4750/20000  lr 2.75e-04  loss 3.438\n",
            "step 4800/20000  lr 2.74e-04  loss 3.190\n",
            "step 4850/20000  lr 2.74e-04  loss 3.310\n",
            "step 4900/20000  lr 2.73e-04  loss 3.211\n",
            "step 4950/20000  lr 2.72e-04  loss 3.165\n",
            "step 5000/20000  lr 2.72e-04  loss 3.111\n",
            "[eval @ step 5000] val_loss 3.153  (52.0s)\n",
            "✓ saved /content/tinygpt_best.pt\n",
            "step 5050/20000  lr 2.71e-04  loss 3.275\n",
            "step 5100/20000  lr 2.70e-04  loss 3.127\n",
            "step 5150/20000  lr 2.69e-04  loss 3.198\n",
            "step 5200/20000  lr 2.69e-04  loss 3.095\n",
            "step 5250/20000  lr 2.68e-04  loss 3.150\n",
            "step 5300/20000  lr 2.67e-04  loss 3.144\n",
            "step 5350/20000  lr 2.67e-04  loss 3.092\n",
            "step 5400/20000  lr 2.66e-04  loss 3.256\n",
            "step 5450/20000  lr 2.65e-04  loss 3.093\n",
            "step 5500/20000  lr 2.64e-04  loss 3.013\n",
            "step 5550/20000  lr 2.64e-04  loss 3.229\n",
            "step 5600/20000  lr 2.63e-04  loss 2.974\n",
            "step 5650/20000  lr 2.62e-04  loss 3.226\n",
            "step 5700/20000  lr 2.61e-04  loss 3.149\n",
            "step 5750/20000  lr 2.60e-04  loss 3.091\n",
            "step 5800/20000  lr 2.60e-04  loss 3.017\n",
            "step 5850/20000  lr 2.59e-04  loss 3.158\n",
            "step 5900/20000  lr 2.58e-04  loss 3.165\n",
            "step 5950/20000  lr 2.57e-04  loss 3.118\n",
            "step 6000/20000  lr 2.56e-04  loss 3.136\n",
            "[eval @ step 6000] val_loss 3.113  (52.7s)\n",
            "✓ saved /content/tinygpt_best.pt\n",
            "step 6050/20000  lr 2.56e-04  loss 3.207\n",
            "step 6100/20000  lr 2.55e-04  loss 3.136\n",
            "step 6150/20000  lr 2.54e-04  loss 3.100\n",
            "step 6200/20000  lr 2.53e-04  loss 3.221\n",
            "step 6250/20000  lr 2.52e-04  loss 3.289\n",
            "step 6300/20000  lr 2.51e-04  loss 3.219\n",
            "step 6350/20000  lr 2.51e-04  loss 2.981\n",
            "step 6400/20000  lr 2.50e-04  loss 2.991\n",
            "step 6450/20000  lr 2.49e-04  loss 3.175\n",
            "step 6500/20000  lr 2.48e-04  loss 3.089\n",
            "step 6550/20000  lr 2.47e-04  loss 3.109\n",
            "step 6600/20000  lr 2.46e-04  loss 3.189\n",
            "step 6650/20000  lr 2.45e-04  loss 3.209\n",
            "step 6700/20000  lr 2.44e-04  loss 3.236\n",
            "step 6750/20000  lr 2.43e-04  loss 3.078\n",
            "step 6800/20000  lr 2.43e-04  loss 3.169\n",
            "step 6850/20000  lr 2.42e-04  loss 3.103\n",
            "step 6900/20000  lr 2.41e-04  loss 3.219\n",
            "step 6950/20000  lr 2.40e-04  loss 3.161\n",
            "step 7000/20000  lr 2.39e-04  loss 3.195\n",
            "[eval @ step 7000] val_loss 3.094  (52.6s)\n",
            "✓ saved /content/tinygpt_best.pt\n",
            "step 7050/20000  lr 2.38e-04  loss 3.163\n",
            "step 7100/20000  lr 2.37e-04  loss 3.115\n",
            "step 7150/20000  lr 2.36e-04  loss 3.121\n",
            "step 7200/20000  lr 2.35e-04  loss 3.148\n",
            "step 7250/20000  lr 2.34e-04  loss 3.208\n",
            "step 7300/20000  lr 2.33e-04  loss 3.075\n",
            "step 7350/20000  lr 2.32e-04  loss 3.119\n",
            "step 7400/20000  lr 2.31e-04  loss 3.100\n",
            "step 7450/20000  lr 2.30e-04  loss 3.233\n",
            "step 7500/20000  lr 2.29e-04  loss 3.257\n",
            "step 7550/20000  lr 2.28e-04  loss 3.151\n",
            "step 7600/20000  lr 2.27e-04  loss 3.119\n",
            "step 7650/20000  lr 2.26e-04  loss 3.089\n",
            "step 7700/20000  lr 2.25e-04  loss 3.154\n",
            "step 7750/20000  lr 2.24e-04  loss 3.062\n",
            "step 7800/20000  lr 2.23e-04  loss 2.997\n",
            "step 7850/20000  lr 2.22e-04  loss 2.980\n",
            "step 7900/20000  lr 2.21e-04  loss 3.192\n",
            "step 7950/20000  lr 2.20e-04  loss 3.138\n",
            "step 8000/20000  lr 2.19e-04  loss 3.162\n",
            "[eval @ step 8000] val_loss 3.081  (52.5s)\n",
            "✓ saved /content/tinygpt_best.pt\n",
            "step 8050/20000  lr 2.18e-04  loss 3.082\n",
            "step 8100/20000  lr 2.17e-04  loss 3.014\n",
            "step 8150/20000  lr 2.16e-04  loss 3.124\n",
            "step 8200/20000  lr 2.15e-04  loss 3.127\n",
            "step 8250/20000  lr 2.14e-04  loss 2.976\n",
            "step 8300/20000  lr 2.13e-04  loss 3.144\n",
            "step 8350/20000  lr 2.12e-04  loss 3.044\n",
            "step 8400/20000  lr 2.11e-04  loss 3.284\n",
            "step 8450/20000  lr 2.10e-04  loss 3.094\n",
            "step 8500/20000  lr 2.09e-04  loss 3.139\n",
            "step 8550/20000  lr 2.08e-04  loss 3.033\n",
            "step 8600/20000  lr 2.07e-04  loss 2.998\n",
            "step 8650/20000  lr 2.06e-04  loss 3.057\n",
            "step 8700/20000  lr 2.05e-04  loss 3.023\n",
            "step 8750/20000  lr 2.04e-04  loss 3.111\n",
            "step 8800/20000  lr 2.02e-04  loss 2.989\n",
            "step 8850/20000  lr 2.01e-04  loss 2.929\n",
            "step 8900/20000  lr 2.00e-04  loss 3.244\n",
            "step 8950/20000  lr 1.99e-04  loss 3.094\n",
            "step 9000/20000  lr 1.98e-04  loss 3.073\n",
            "[eval @ step 9000] val_loss 3.075  (52.7s)\n",
            "✓ saved /content/tinygpt_best.pt\n",
            "step 9050/20000  lr 1.97e-04  loss 3.058\n",
            "step 9100/20000  lr 1.96e-04  loss 3.022\n",
            "step 9150/20000  lr 1.95e-04  loss 3.176\n",
            "step 9200/20000  lr 1.94e-04  loss 3.131\n",
            "step 9250/20000  lr 1.93e-04  loss 3.086\n",
            "step 9300/20000  lr 1.92e-04  loss 3.217\n",
            "step 9350/20000  lr 1.91e-04  loss 2.937\n",
            "step 9400/20000  lr 1.89e-04  loss 3.069\n",
            "step 9450/20000  lr 1.88e-04  loss 3.136\n",
            "step 9500/20000  lr 1.87e-04  loss 3.131\n",
            "step 9550/20000  lr 1.86e-04  loss 3.043\n",
            "step 9600/20000  lr 1.85e-04  loss 3.034\n",
            "step 9650/20000  lr 1.84e-04  loss 3.137\n",
            "step 9700/20000  lr 1.83e-04  loss 3.065\n",
            "step 9750/20000  lr 1.82e-04  loss 3.111\n",
            "step 9800/20000  lr 1.81e-04  loss 3.065\n",
            "step 9850/20000  lr 1.79e-04  loss 3.198\n",
            "step 9900/20000  lr 1.78e-04  loss 3.042\n",
            "step 9950/20000  lr 1.77e-04  loss 3.091\n",
            "step 10000/20000  lr 1.76e-04  loss 3.059\n",
            "[eval @ step 10000] val_loss 3.071  (52.6s)\n",
            "✓ saved /content/tinygpt_best.pt\n",
            "step 10050/20000  lr 1.75e-04  loss 3.062\n",
            "step 10100/20000  lr 1.74e-04  loss 3.111\n",
            "step 10150/20000  lr 1.73e-04  loss 3.141\n",
            "step 10200/20000  lr 1.72e-04  loss 3.189\n",
            "step 10250/20000  lr 1.71e-04  loss 2.982\n",
            "step 10300/20000  lr 1.69e-04  loss 3.274\n",
            "step 10350/20000  lr 1.68e-04  loss 3.068\n",
            "step 10400/20000  lr 1.67e-04  loss 3.027\n",
            "step 10450/20000  lr 1.66e-04  loss 3.010\n",
            "step 10500/20000  lr 1.65e-04  loss 3.099\n",
            "step 10550/20000  lr 1.64e-04  loss 3.091\n",
            "step 10600/20000  lr 1.63e-04  loss 3.123\n",
            "step 10650/20000  lr 1.62e-04  loss 3.109\n",
            "step 10700/20000  lr 1.61e-04  loss 3.071\n",
            "step 10750/20000  lr 1.59e-04  loss 2.944\n",
            "step 10800/20000  lr 1.58e-04  loss 2.990\n",
            "step 10850/20000  lr 1.57e-04  loss 3.044\n",
            "step 10900/20000  lr 1.56e-04  loss 3.177\n",
            "step 10950/20000  lr 1.55e-04  loss 3.192\n",
            "step 11000/20000  lr 1.54e-04  loss 3.047\n",
            "[eval @ step 11000] val_loss 3.067  (52.7s)\n",
            "✓ saved /content/tinygpt_best.pt\n",
            "step 11050/20000  lr 1.53e-04  loss 3.065\n",
            "step 11100/20000  lr 1.52e-04  loss 3.077\n",
            "step 11150/20000  lr 1.51e-04  loss 3.103\n",
            "step 11200/20000  lr 1.49e-04  loss 3.188\n",
            "step 11250/20000  lr 1.48e-04  loss 3.062\n",
            "step 11300/20000  lr 1.47e-04  loss 3.074\n",
            "step 11350/20000  lr 1.46e-04  loss 3.183\n",
            "step 11400/20000  lr 1.45e-04  loss 3.053\n",
            "step 11450/20000  lr 1.44e-04  loss 2.978\n",
            "step 11500/20000  lr 1.43e-04  loss 3.011\n",
            "step 11550/20000  lr 1.42e-04  loss 3.106\n",
            "step 11600/20000  lr 1.41e-04  loss 3.077\n",
            "step 11650/20000  lr 1.39e-04  loss 3.182\n",
            "step 11700/20000  lr 1.38e-04  loss 3.053\n",
            "step 11750/20000  lr 1.37e-04  loss 3.225\n",
            "step 11800/20000  lr 1.36e-04  loss 3.121\n",
            "step 11850/20000  lr 1.35e-04  loss 2.947\n",
            "step 11900/20000  lr 1.34e-04  loss 3.192\n",
            "step 11950/20000  lr 1.33e-04  loss 3.218\n",
            "step 12000/20000  lr 1.32e-04  loss 2.924\n",
            "[eval @ step 12000] val_loss 3.065  (52.6s)\n",
            "✓ saved /content/tinygpt_best.pt\n",
            "step 12050/20000  lr 1.31e-04  loss 3.135\n",
            "step 12100/20000  lr 1.30e-04  loss 3.004\n",
            "step 12150/20000  lr 1.29e-04  loss 3.114\n",
            "step 12200/20000  lr 1.28e-04  loss 3.183\n",
            "step 12250/20000  lr 1.26e-04  loss 3.193\n",
            "step 12300/20000  lr 1.25e-04  loss 3.069\n",
            "step 12350/20000  lr 1.24e-04  loss 2.972\n",
            "step 12400/20000  lr 1.23e-04  loss 3.034\n",
            "step 12450/20000  lr 1.22e-04  loss 3.093\n",
            "step 12500/20000  lr 1.21e-04  loss 3.235\n",
            "step 12550/20000  lr 1.20e-04  loss 2.938\n",
            "step 12600/20000  lr 1.19e-04  loss 3.040\n",
            "step 12650/20000  lr 1.18e-04  loss 3.092\n",
            "step 12700/20000  lr 1.17e-04  loss 3.146\n",
            "step 12750/20000  lr 1.16e-04  loss 3.152\n",
            "step 12800/20000  lr 1.15e-04  loss 2.891\n",
            "step 12850/20000  lr 1.14e-04  loss 3.084\n",
            "step 12900/20000  lr 1.13e-04  loss 3.001\n",
            "step 12950/20000  lr 1.12e-04  loss 3.151\n",
            "step 13000/20000  lr 1.11e-04  loss 3.088\n",
            "[eval @ step 13000] val_loss 3.062  (52.8s)\n",
            "✓ saved /content/tinygpt_best.pt\n",
            "step 13050/20000  lr 1.10e-04  loss 2.920\n",
            "step 13100/20000  lr 1.09e-04  loss 3.215\n",
            "step 13150/20000  lr 1.08e-04  loss 2.968\n",
            "step 13200/20000  lr 1.07e-04  loss 3.178\n",
            "step 13250/20000  lr 1.06e-04  loss 3.197\n",
            "step 13300/20000  lr 1.05e-04  loss 3.054\n",
            "step 13350/20000  lr 1.04e-04  loss 3.220\n",
            "step 13400/20000  lr 1.03e-04  loss 3.026\n",
            "step 13450/20000  lr 1.02e-04  loss 2.996\n",
            "step 13500/20000  lr 1.01e-04  loss 3.011\n",
            "step 13550/20000  lr 9.98e-05  loss 3.000\n",
            "step 13600/20000  lr 9.88e-05  loss 2.942\n",
            "step 13650/20000  lr 9.78e-05  loss 3.045\n",
            "step 13700/20000  lr 9.69e-05  loss 2.977\n",
            "step 13750/20000  lr 9.59e-05  loss 3.120\n",
            "step 13800/20000  lr 9.49e-05  loss 2.933\n",
            "step 13850/20000  lr 9.40e-05  loss 3.038\n",
            "step 13900/20000  lr 9.30e-05  loss 3.039\n",
            "step 13950/20000  lr 9.21e-05  loss 3.071\n",
            "step 14000/20000  lr 9.12e-05  loss 3.076\n",
            "[eval @ step 14000] val_loss 3.060  (52.5s)\n",
            "✓ saved /content/tinygpt_best.pt\n",
            "step 14050/20000  lr 9.02e-05  loss 2.981\n",
            "step 14100/20000  lr 8.93e-05  loss 3.079\n",
            "step 14150/20000  lr 8.84e-05  loss 2.872\n",
            "step 14200/20000  lr 8.75e-05  loss 2.957\n",
            "step 14250/20000  lr 8.66e-05  loss 3.213\n",
            "step 14300/20000  lr 8.56e-05  loss 3.233\n",
            "step 14350/20000  lr 8.47e-05  loss 3.067\n",
            "step 14400/20000  lr 8.39e-05  loss 3.076\n",
            "step 14450/20000  lr 8.30e-05  loss 3.282\n",
            "step 14500/20000  lr 8.21e-05  loss 2.967\n",
            "step 14550/20000  lr 8.12e-05  loss 3.086\n",
            "step 14600/20000  lr 8.03e-05  loss 3.216\n",
            "step 14650/20000  lr 7.95e-05  loss 3.051\n",
            "step 14700/20000  lr 7.86e-05  loss 3.053\n",
            "step 14750/20000  lr 7.77e-05  loss 3.040\n",
            "step 14800/20000  lr 7.69e-05  loss 3.105\n",
            "step 14850/20000  lr 7.61e-05  loss 3.136\n",
            "step 14900/20000  lr 7.52e-05  loss 3.045\n",
            "step 14950/20000  lr 7.44e-05  loss 3.052\n",
            "step 15000/20000  lr 7.36e-05  loss 2.965\n",
            "[eval @ step 15000] val_loss 3.059  (52.5s)\n",
            "✓ saved /content/tinygpt_best.pt\n",
            "step 15050/20000  lr 7.27e-05  loss 3.139\n",
            "step 15100/20000  lr 7.19e-05  loss 3.134\n",
            "step 15150/20000  lr 7.11e-05  loss 3.016\n",
            "step 15200/20000  lr 7.03e-05  loss 3.140\n",
            "step 15250/20000  lr 6.95e-05  loss 3.032\n",
            "step 15300/20000  lr 6.88e-05  loss 2.973\n",
            "step 15350/20000  lr 6.80e-05  loss 3.063\n",
            "step 15400/20000  lr 6.72e-05  loss 3.079\n",
            "step 15450/20000  lr 6.64e-05  loss 3.132\n",
            "step 15500/20000  lr 6.57e-05  loss 2.977\n",
            "step 15550/20000  lr 6.49e-05  loss 3.091\n",
            "step 15600/20000  lr 6.42e-05  loss 3.121\n",
            "step 15650/20000  lr 6.34e-05  loss 3.000\n",
            "step 15700/20000  lr 6.27e-05  loss 3.105\n",
            "step 15750/20000  lr 6.20e-05  loss 3.076\n",
            "step 15800/20000  lr 6.13e-05  loss 3.163\n",
            "step 15850/20000  lr 6.06e-05  loss 3.018\n",
            "step 15900/20000  lr 5.99e-05  loss 3.066\n",
            "step 15950/20000  lr 5.92e-05  loss 3.111\n",
            "step 16000/20000  lr 5.85e-05  loss 2.977\n",
            "[eval @ step 16000] val_loss 3.057  (52.4s)\n",
            "✓ saved /content/tinygpt_best.pt\n",
            "step 16050/20000  lr 5.78e-05  loss 2.878\n",
            "step 16100/20000  lr 5.71e-05  loss 3.037\n",
            "step 16150/20000  lr 5.64e-05  loss 2.997\n",
            "step 16200/20000  lr 5.58e-05  loss 2.960\n",
            "step 16250/20000  lr 5.51e-05  loss 3.005\n",
            "step 16300/20000  lr 5.45e-05  loss 3.076\n",
            "step 16350/20000  lr 5.38e-05  loss 2.943\n",
            "step 16400/20000  lr 5.32e-05  loss 3.111\n",
            "step 16450/20000  lr 5.26e-05  loss 3.010\n",
            "step 16500/20000  lr 5.20e-05  loss 3.020\n",
            "step 16550/20000  lr 5.14e-05  loss 3.100\n",
            "step 16600/20000  lr 5.08e-05  loss 2.998\n",
            "step 16650/20000  lr 5.02e-05  loss 3.033\n",
            "step 16700/20000  lr 4.96e-05  loss 3.098\n",
            "step 16750/20000  lr 4.90e-05  loss 3.044\n",
            "step 16800/20000  lr 4.85e-05  loss 3.196\n",
            "step 16850/20000  lr 4.79e-05  loss 2.992\n",
            "step 16900/20000  lr 4.73e-05  loss 2.917\n",
            "step 16950/20000  lr 4.68e-05  loss 3.064\n",
            "step 17000/20000  lr 4.63e-05  loss 2.963\n",
            "[eval @ step 17000] val_loss 3.057  (52.6s)\n",
            "✓ saved /content/tinygpt_best.pt\n",
            "step 17050/20000  lr 4.57e-05  loss 3.193\n",
            "step 17100/20000  lr 4.52e-05  loss 3.045\n",
            "step 17150/20000  lr 4.47e-05  loss 2.982\n",
            "step 17200/20000  lr 4.42e-05  loss 3.092\n",
            "step 17250/20000  lr 4.37e-05  loss 3.071\n",
            "step 17300/20000  lr 4.32e-05  loss 3.090\n",
            "step 17350/20000  lr 4.28e-05  loss 2.968\n",
            "step 17400/20000  lr 4.23e-05  loss 2.962\n",
            "step 17450/20000  lr 4.18e-05  loss 2.975\n",
            "step 17500/20000  lr 4.14e-05  loss 3.028\n",
            "step 17550/20000  lr 4.09e-05  loss 3.009\n",
            "step 17600/20000  lr 4.05e-05  loss 3.039\n",
            "step 17650/20000  lr 4.01e-05  loss 2.969\n",
            "step 17700/20000  lr 3.96e-05  loss 3.043\n",
            "step 17750/20000  lr 3.92e-05  loss 2.923\n",
            "step 17800/20000  lr 3.88e-05  loss 2.891\n",
            "step 17850/20000  lr 3.84e-05  loss 3.133\n",
            "step 17900/20000  lr 3.81e-05  loss 3.054\n",
            "step 17950/20000  lr 3.77e-05  loss 2.953\n",
            "step 18000/20000  lr 3.73e-05  loss 3.193\n",
            "[eval @ step 18000] val_loss 3.056  (52.8s)\n",
            "✓ saved /content/tinygpt_best.pt\n",
            "step 18050/20000  lr 3.70e-05  loss 2.975\n",
            "step 18100/20000  lr 3.66e-05  loss 3.103\n",
            "step 18150/20000  lr 3.63e-05  loss 2.974\n",
            "step 18200/20000  lr 3.59e-05  loss 3.072\n",
            "step 18250/20000  lr 3.56e-05  loss 2.970\n",
            "step 18300/20000  lr 3.53e-05  loss 3.154\n",
            "step 18350/20000  lr 3.50e-05  loss 3.152\n",
            "step 18400/20000  lr 3.47e-05  loss 3.060\n",
            "step 18450/20000  lr 3.44e-05  loss 3.154\n",
            "step 18500/20000  lr 3.41e-05  loss 3.150\n",
            "step 18550/20000  lr 3.39e-05  loss 3.143\n",
            "step 18600/20000  lr 3.36e-05  loss 3.054\n",
            "step 18650/20000  lr 3.33e-05  loss 3.098\n",
            "step 18700/20000  lr 3.31e-05  loss 3.166\n",
            "step 18750/20000  lr 3.29e-05  loss 3.153\n",
            "step 18800/20000  lr 3.26e-05  loss 3.133\n",
            "step 18850/20000  lr 3.24e-05  loss 2.910\n",
            "step 18900/20000  lr 3.22e-05  loss 3.109\n",
            "step 18950/20000  lr 3.20e-05  loss 3.075\n",
            "step 19000/20000  lr 3.18e-05  loss 3.054\n",
            "[eval @ step 19000] val_loss 3.055  (52.7s)\n",
            "✓ saved /content/tinygpt_best.pt\n",
            "step 19050/20000  lr 3.17e-05  loss 3.065\n",
            "step 19100/20000  lr 3.15e-05  loss 3.022\n",
            "step 19150/20000  lr 3.13e-05  loss 3.107\n",
            "step 19200/20000  lr 3.12e-05  loss 3.018\n",
            "step 19250/20000  lr 3.10e-05  loss 3.026\n",
            "step 19300/20000  lr 3.09e-05  loss 3.190\n",
            "step 19350/20000  lr 3.08e-05  loss 3.126\n",
            "step 19400/20000  lr 3.07e-05  loss 3.203\n",
            "step 19450/20000  lr 3.06e-05  loss 3.047\n",
            "step 19500/20000  lr 3.05e-05  loss 3.018\n",
            "step 19550/20000  lr 3.04e-05  loss 2.968\n",
            "step 19600/20000  lr 3.03e-05  loss 3.004\n",
            "step 19650/20000  lr 3.02e-05  loss 3.015\n",
            "step 19700/20000  lr 3.02e-05  loss 3.077\n",
            "step 19750/20000  lr 3.01e-05  loss 3.086\n",
            "step 19800/20000  lr 3.01e-05  loss 3.163\n",
            "step 19850/20000  lr 3.00e-05  loss 3.108\n",
            "step 19900/20000  lr 3.00e-05  loss 3.016\n",
            "step 19950/20000  lr 3.00e-05  loss 2.956\n",
            "step 20000/20000  lr 3.00e-05  loss 3.019\n",
            "[eval @ step 20000] val_loss 3.055  (52.5s)\n",
            "✓ saved /content/tinygpt_best.pt\n",
            "Loaded best checkpoint.\n",
            "You: Hi, how are you?\n",
            "Bot: I’m wondering if I ' Ve got a job with my daughter. My mother was coming and yet. And for them very far until he was able to get out of the last time after breakfast and you wanted me and so I didn ' t know that, but I will be wonderful.\n",
            "----\n",
            "You: What's your favorite movie?\n",
            "Bot: I think she is very nervous. They don't really like it! She's a lot of time to buy this birthday ; I'm taking food in the visit and for work at home.\n"
          ]
        }
      ],
      "source": [
        "# ================================\n",
        "# 0) Setup (installs + imports)\n",
        "# ================================\n",
        "!pip -q install datasets sentencepiece\n",
        "\n",
        "import os, math, time, re, random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from datasets import load_dataset\n",
        "import sentencepiece as spm\n",
        "\n",
        "# ---- Hyperparams / knobs ----\n",
        "block_size   = 128     # try 192/256 later if memory allows (must retrain pos-embed)\n",
        "batch_size   = 64      # lower to 32/16 if OOM\n",
        "base_lr      = 3e-4\n",
        "warmup_steps = 1_000\n",
        "max_steps    = 20_000    # bump to 20_000+ for better quality\n",
        "eval_every   = 1_000\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# Optional speedups\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "if hasattr(torch, \"set_float32_matmul_precision\"):\n",
        "    torch.set_float32_matmul_precision(\"high\")\n",
        "\n",
        "# ================================\n",
        "# 1) Load DailyDialog & format with <A>/<B>/<eot>\n",
        "# ================================\n",
        "raw = load_dataset(\"elricwan/dailydialog\")  # public parquet mirror, no token needed\n",
        "print(raw)\n",
        "\n",
        "if \"validation\" in raw:\n",
        "    ds = {\"train\": raw[\"train\"], \"validation\": raw[\"validation\"]}\n",
        "else:\n",
        "    splits = raw[\"train\"].train_test_split(test_size=0.1, seed=1337)\n",
        "    ds = {\"train\": splits[\"train\"], \"validation\": splits[\"test\"]}\n",
        "\n",
        "print(\"Columns (train):\", ds[\"train\"].column_names)\n",
        "\n",
        "# ---- Normalization helpers ----\n",
        "ROLE_PREFIX_RE = re.compile(r'^(?:<A>|<B>|Person\\s+[AB]:|[AB]:)\\s*', flags=re.IGNORECASE)\n",
        "\n",
        "def strip_role_prefixes(s: str) -> str:\n",
        "    return ROLE_PREFIX_RE.sub('', s.strip())\n",
        "\n",
        "def clean_sentence(s: str) -> str:\n",
        "    s = s.strip()\n",
        "    s = (s.replace(\" ,\", \",\").replace(\" .\", \".\").replace(\" !\", \"!\").replace(\" ?\", \"?\")\n",
        "           .replace(\" ' s\",\"’s\").replace(\" ' m\",\"’m\").replace(\" ' ve\",\"’ve\").replace(\" ' re\",\"’re\"))\n",
        "    return s\n",
        "\n",
        "def row_to_dialog_list(row):\n",
        "    for key in (\"dialog\",\"dialogs\",\"utterances\",\"texts\",\"conversation\",\"conversations\",\"data\"):\n",
        "        if key in row:\n",
        "            val = row[key]; break\n",
        "    else:\n",
        "        k0 = next(iter(row.keys())); val = row[k0]\n",
        "    if isinstance(val, list):\n",
        "        if val and isinstance(val[0], str): return val\n",
        "        if val and isinstance(val[0], dict):\n",
        "            for kk in (\"utterance\",\"text\",\"content\"):\n",
        "                if kk in val[0]: return [u.get(kk, \"\") for u in val]\n",
        "            return [str(u) for u in val]\n",
        "    return [str(val)]\n",
        "\n",
        "def dialog_to_text_row(row):\n",
        "    sents = row_to_dialog_list(row)\n",
        "    lines = []\n",
        "    for i, s in enumerate(sents):\n",
        "        role = \"<A>\" if (i % 2 == 0) else \"<B>\"\n",
        "        utt = clean_sentence(strip_role_prefixes(s))\n",
        "        lines.append(f\"{role} {utt} <eot>\")\n",
        "    return \"<bos>\\n\" + \"\\n\".join(lines) + \"\\n<eos>\"\n",
        "\n",
        "train_texts = [dialog_to_text_row(r) for r in ds[\"train\"]]\n",
        "val_texts   = [dialog_to_text_row(r) for r in ds[\"validation\"]]\n",
        "\n",
        "print(\"Preview (formatted):\\n\", train_texts[0][:500])\n",
        "\n",
        "# ================================\n",
        "# 1b) Validate format & drop malformed dialogs\n",
        "# ================================\n",
        "LINE_RE = re.compile(r'^(<A>|<B>)\\s.+\\s<eot>$')\n",
        "\n",
        "def validate_dialog(text: str):\n",
        "    errs = []\n",
        "    if not text.startswith(\"<bos>\\n\"): errs.append(\"missing <bos> at start\")\n",
        "    if not text.rstrip().endswith(\"<eos>\"): errs.append(\"missing <eos> at end\")\n",
        "    body = [ln for ln in text.splitlines() if ln not in (\"<bos>\", \"<eos>\")]\n",
        "    if not body: errs.append(\"empty dialog body\"); return errs\n",
        "    prev_speaker = None\n",
        "    for i, ln in enumerate(body):\n",
        "        m = LINE_RE.match(ln)\n",
        "        if not m:\n",
        "            errs.append(f\"line {i} malformed: {ln[:120]}\")\n",
        "            continue\n",
        "        spk = m.group(1)\n",
        "        utt = ln[len(spk):].strip()\n",
        "        if \"Person A:\" in utt or \"Person B:\" in utt:\n",
        "            errs.append(f\"line {i} literal 'Person X:' leaked\")\n",
        "        if prev_speaker == spk:\n",
        "            errs.append(f\"consecutive same speaker at lines {i-1},{i}\")\n",
        "        prev_speaker = spk\n",
        "    return errs\n",
        "\n",
        "def filter_bad(texts):\n",
        "    keep = []\n",
        "    bad = 0\n",
        "    for t in texts:\n",
        "        e = validate_dialog(t)\n",
        "        if e:\n",
        "            bad += 1\n",
        "        else:\n",
        "            keep.append(t)\n",
        "    return keep, bad\n",
        "\n",
        "train_texts, bad_train = filter_bad(train_texts)\n",
        "val_texts,   bad_val   = filter_bad(val_texts)\n",
        "print(f\"Validation — dropped: train {bad_train}, val {bad_val}\")\n",
        "print(f\"Final counts — train {len(train_texts)}, val {len(val_texts)}\")\n",
        "\n",
        "# ================================\n",
        "# 2) Train SentencePiece tokenizer (BPE, 8k vocab)\n",
        "# ================================\n",
        "corpus_path = \"/content/dailydialog_corpus.txt\"\n",
        "with open(corpus_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    for t in train_texts:\n",
        "        f.write(t + \"\\n\")\n",
        "\n",
        "!ls -lh /content/dailydialog_corpus.txt\n",
        "!head -n 5 /content/dailydialog_corpus.txt\n",
        "\n",
        "# clean slate tokenizer\n",
        "!rm -f /content/spm_dd_4k.model /content/spm_dd_4k.vocab\n",
        "\n",
        "spm.SentencePieceTrainer.Train(\n",
        "    input=corpus_path,\n",
        "    model_prefix=\"/content/spm_dd_4k\",\n",
        "    vocab_size=8000,                 # 8k usually reads nicer than 4k for this data\n",
        "    model_type=\"bpe\",\n",
        "    character_coverage=1.0,\n",
        "    pad_id=0, unk_id=1, bos_id=2, eos_id=3,   # reserve core specials\n",
        "    user_defined_symbols=[\"<bos>\",\"<eos>\",\"<A>\",\"<B>\",\"<eot>\"]  # single-token tags\n",
        ")\n",
        "\n",
        "sp = spm.SentencePieceProcessor(model_file=\"/content/spm_dd_4k.model\")\n",
        "vocab_size = sp.vocab_size()\n",
        "pad_id, unk_id, bos_id, eos_id = 0, 1, 2, 3\n",
        "print(\"Vocab size:\", vocab_size, \" (pad,unk,bos,eos) =\", (pad_id,unk_id,bos_id,eos_id))\n",
        "for tok in [\"<bos>\", \"<eos>\", \"<A>\", \"<B>\", \"<eot>\"]:\n",
        "    print(tok, \"->\", sp.piece_to_id(tok))\n",
        "\n",
        "# ================================\n",
        "# 3) Encode dataset → LM chunks\n",
        "# ================================\n",
        "def encode_texts(texts):\n",
        "    return np.array([tid for t in texts for tid in sp.encode(t, out_type=int)], dtype=np.int32)\n",
        "\n",
        "train_ids = encode_texts(train_texts)\n",
        "val_ids   = encode_texts(val_texts)\n",
        "print(\"Encoded lengths:\", len(train_ids), len(val_ids))\n",
        "\n",
        "class LMChunkDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, ids, block_size):\n",
        "        L = (len(ids) - 1) // block_size\n",
        "        self.input  = torch.tensor(ids[:L*block_size],    dtype=torch.long).view(L, block_size)\n",
        "        self.target = torch.tensor(ids[1:L*block_size+1], dtype=torch.long).view(L, block_size)\n",
        "    def __len__(self): return self.input.size(0)\n",
        "    def __getitem__(self, idx): return self.input[idx], self.target[idx]\n",
        "\n",
        "train_ds = LMChunkDataset(train_ids, block_size)\n",
        "val_ds   = LMChunkDataset(val_ids, block_size)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True,  drop_last=True)\n",
        "val_loader   = torch.utils.data.DataLoader(val_ds,   batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "print(\"Batches:\", len(train_ds), len(val_ds))\n",
        "\n",
        "# ================================\n",
        "# 4) Define a tiny GPT (~5M params)\n",
        "# ================================\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, d_model=256, n_heads=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0\n",
        "        self.n_heads = n_heads\n",
        "        self.d_head  = d_model // n_heads\n",
        "        self.qkv   = nn.Linear(d_model, 3*d_model, bias=False)\n",
        "        self.proj  = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.attn_drop  = nn.Dropout(dropout)\n",
        "        self.resid_drop = nn.Dropout(dropout)\n",
        "        self.register_buffer(\"mask\", None, persistent=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        if (self.mask is None) or (self.mask.size(-1) < T):\n",
        "            self.mask = torch.tril(torch.ones(T, T, device=x.device)).view(1,1,T,T)\n",
        "        qkv = self.qkv(x)                            # (B,T,3C)\n",
        "        q, k, v = qkv.split(C, dim=2)\n",
        "        def split_heads(t): return t.view(B, T, self.n_heads, self.d_head).transpose(1,2)\n",
        "        q, k, v = map(split_heads, (q, k, v))        # (B,H,T,Dh)\n",
        "        att = (q @ k.transpose(-2, -1)) / (self.d_head ** 0.5)\n",
        "        att = att.masked_fill(self.mask[:, :, :T, :T] == 0, float(\"-inf\"))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attn_drop(att)\n",
        "        y = att @ v                                  # (B,H,T,Dh)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.resid_drop(self.proj(y))\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, d_model=256, d_ff=1024, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "    def forward(self, x):\n",
        "        return self.drop(self.fc2(self.act(self.fc1(x))))\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, d_model=256, n_heads=4, d_ff=1024, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.attn = CausalSelfAttention(d_model, n_heads, dropout)\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "        self.mlp  = MLP(d_model, d_ff, dropout)\n",
        "        self.drop_res = nn.Dropout(dropout)\n",
        "    def forward(self, x):\n",
        "        x = x + self.drop_res(self.attn(self.ln1(x)))\n",
        "        x = x + self.drop_res(self.mlp(self.ln2(x)))\n",
        "        return x\n",
        "\n",
        "class TinyGPT(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=256, n_heads=4, d_ff=1024, n_layers=5, max_seq_len=128, dropout=0.1, pad_id=0):\n",
        "        super().__init__()\n",
        "        self.token_embed = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n",
        "        self.pos_embed   = nn.Embedding(max_seq_len, d_model)\n",
        "        self.drop  = nn.Dropout(dropout)\n",
        "        self.blocks = nn.ModuleList([Block(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)])\n",
        "        self.ln_f  = nn.LayerNorm(d_model)\n",
        "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
        "        # weight tying\n",
        "        self.lm_head.weight = self.token_embed.weight\n",
        "        self.apply(self._init)\n",
        "\n",
        "    @staticmethod\n",
        "    def _init(m):\n",
        "        if isinstance(m, (nn.Linear, nn.Embedding)):\n",
        "            nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
        "        if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "            nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, idx):\n",
        "        B, T = idx.size()\n",
        "        pos = torch.arange(0, T, device=idx.device).unsqueeze(0)\n",
        "        x = self.token_embed(idx) + self.pos_embed(pos)\n",
        "        x = self.drop(x)\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "        x = self.ln_f(x)\n",
        "        return self.lm_head(x)\n",
        "\n",
        "model = TinyGPT(vocab_size=vocab_size, d_model=256, n_heads=4, d_ff=1024,\n",
        "                n_layers=5, max_seq_len=block_size, dropout=0.1, pad_id=pad_id).to(device)\n",
        "\n",
        "n_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total parameters: {n_params/1e6:.2f}M\")\n",
        "\n",
        "# ================================\n",
        "# 5) Train (AdamW, warmup + cosine, AMP new API, grad clip)\n",
        "# ================================\n",
        "def get_lr(step, warmup, max_steps, base_lr):\n",
        "    if step < warmup:\n",
        "        return base_lr * step / max(1, warmup)\n",
        "    progress = (step - warmup) / max(1, (max_steps - warmup))\n",
        "    return 0.1*base_lr + 0.9*base_lr * 0.5 * (1 + math.cos(math.pi * progress))\n",
        "\n",
        "# (Optional) better weight decay: don't decay LayerNorm/bias\n",
        "decay, no_decay = [], []\n",
        "for n,p in model.named_parameters():\n",
        "    if not p.requires_grad: continue\n",
        "    if n.endswith(\"bias\") or \"ln\" in n.lower() or \"layernorm\" in n.lower():\n",
        "        no_decay.append(p)\n",
        "    else:\n",
        "        decay.append(p)\n",
        "optimizer = torch.optim.AdamW(\n",
        "    [{\"params\": decay,    \"weight_decay\": 0.1},\n",
        "     {\"params\": no_decay, \"weight_decay\": 0.0}],\n",
        "    lr=base_lr, betas=(0.9, 0.95)\n",
        ")\n",
        "\n",
        "scaler = torch.amp.GradScaler(\"cuda\", enabled=(device==\"cuda\"))\n",
        "\n",
        "def run_eval():\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in val_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            with torch.amp.autocast(\"cuda\", enabled=(device==\"cuda\")):\n",
        "                logits = model(xb)\n",
        "                loss = F.cross_entropy(logits.view(-1, vocab_size), yb.view(-1), ignore_index=pad_id)\n",
        "            losses.append(loss.item())\n",
        "    model.train()\n",
        "    return sum(losses)/len(losses) if losses else float(\"nan\")\n",
        "\n",
        "model.train()\n",
        "global_step = 0\n",
        "best_val = float(\"inf\")\n",
        "t0 = time.time()\n",
        "ckpt_path = \"/content/tinygpt_best.pt\"\n",
        "\n",
        "for epoch in range(999999):\n",
        "    for xb, yb in train_loader:\n",
        "        global_step += 1\n",
        "        lr = get_lr(global_step, warmup_steps, max_steps, base_lr)\n",
        "        for pg in optimizer.param_groups: pg[\"lr\"] = lr\n",
        "\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with torch.amp.autocast(\"cuda\", enabled=(device==\"cuda\")):\n",
        "            logits = model(xb)\n",
        "            loss = F.cross_entropy(logits.view(-1, vocab_size), yb.view(-1), ignore_index=pad_id)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        if global_step % 50 == 0:\n",
        "            print(f\"step {global_step}/{max_steps}  lr {lr:.2e}  loss {loss.item():.3f}\")\n",
        "\n",
        "        if global_step % eval_every == 0:\n",
        "            val_loss = run_eval()\n",
        "            dt = time.time() - t0\n",
        "            print(f\"[eval @ step {global_step}] val_loss {val_loss:.3f}  ({dt:.1f}s)\")\n",
        "            t0 = time.time()\n",
        "            if val_loss < best_val:\n",
        "                best_val = val_loss\n",
        "                torch.save({\"model\": model.state_dict(),\n",
        "                            \"config\": {\"vocab_size\": vocab_size, \"block_size\": block_size}},\n",
        "                           ckpt_path)\n",
        "                print(\"✓ saved\", ckpt_path)\n",
        "\n",
        "        if global_step >= max_steps:\n",
        "            break\n",
        "    if global_step >= max_steps:\n",
        "        break\n",
        "\n",
        "# ================================\n",
        "# 6) Sampling (multi-token stops + no-repeat-3gram)\n",
        "# ================================\n",
        "def detok_cleanup(txt: str) -> str:\n",
        "    return (txt.replace(\" ,\", \",\").replace(\" .\", \".\").replace(\" !\", \"!\")\n",
        "              .replace(\" ?\", \"?\").replace(\" ’ \", \"’\")\n",
        "              .replace(\" ' s\",\"’s\").replace(\" ' m\",\"’m\")\n",
        "              .replace(\" ' ve\",\"’ve\").replace(\" ' re\",\"’re\")\n",
        "              .replace(\" ' d\",\"’d\").replace(\" ' ll\",\"’ll\").replace(\" n't\",\"n’t\")).strip()\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample(\n",
        "    model, sp, prompt,\n",
        "    max_new_tokens=120,\n",
        "    temperature=0.80,\n",
        "    top_p=0.95,\n",
        "    min_tokens_before_stop=24,\n",
        "    repetition_penalty=1.15,\n",
        "    penalty_ctx=80,\n",
        "    include_prompt=False\n",
        "):\n",
        "    model.eval()\n",
        "\n",
        "    # seed as dialogue turn using single-token tags\n",
        "    seed = \"<bos>\\n<A> \" + prompt.strip() + \" <eot>\\n<B> \"\n",
        "    x = torch.tensor(sp.encode(seed, out_type=int), dtype=torch.long, device=device)[None, ...]\n",
        "    start_len = x.size(1)\n",
        "\n",
        "    # stop sequences as token lists\n",
        "    STOP_STRINGS = [\"<eot>\", \"<eos>\", \"<bos>\", \"<A>\", \"<B>\"]\n",
        "    STOP_SEQS = [sp.encode(s, out_type=int) for s in STOP_STRINGS]\n",
        "    FIRST_TOKENS = {seq[0] for seq in STOP_SEQS if len(seq) > 0}\n",
        "\n",
        "    def ends_with(seq, suffix):\n",
        "        L = len(suffix)\n",
        "        return L > 0 and len(seq) >= L and seq[-L:] == suffix\n",
        "\n",
        "    def find_first_stop(gen_ids):\n",
        "        N = len(gen_ids)\n",
        "        cut = N\n",
        "        for i in range(N):\n",
        "            for s in STOP_SEQS:\n",
        "                L = len(s)\n",
        "                if L and i+L <= N and gen_ids[i:i+L] == s:\n",
        "                    cut = min(cut, i)\n",
        "        return cut\n",
        "\n",
        "    def block_repeated_ngrams(logits_row, full_ids, n=3, window=256):\n",
        "        full_ids = full_ids[-window:]\n",
        "        if len(full_ids) < n-1: return\n",
        "        prev = tuple(full_ids[-(n-1):])\n",
        "        forbidden = set()\n",
        "        for i in range(len(full_ids)-(n-1)):\n",
        "            if tuple(full_ids[i:i+n-1]) == prev:\n",
        "                forbidden.add(full_ids[i+n-1])\n",
        "        for t in forbidden:\n",
        "            logits_row[t] = -float(\"inf\")\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        if x.size(1) > block_size:\n",
        "            x = x[:, -block_size:]\n",
        "\n",
        "        with torch.amp.autocast(\"cuda\", enabled=(device==\"cuda\")):\n",
        "            logits = model(x)[:, -1, :]\n",
        "\n",
        "        # temperature\n",
        "        logits = logits / max(1e-8, temperature)\n",
        "\n",
        "        # repetition penalty on recent context\n",
        "        if repetition_penalty and penalty_ctx > 0:\n",
        "            recent = x[0, max(0, x.size(1) - penalty_ctx):].tolist()\n",
        "            for t in set(recent):\n",
        "                logits[0, t] /= repetition_penalty\n",
        "\n",
        "        # block repeated 3-grams\n",
        "        block_repeated_ngrams(logits[0], x[0].tolist(), n=3, window=block_size)\n",
        "\n",
        "        # before min length, forbid starting any stop sequence\n",
        "        gen_len = x.size(1) - start_len\n",
        "        if gen_len < min_tokens_before_stop:\n",
        "            for t0 in FIRST_TOKENS:\n",
        "                logits[0, t0] = -float(\"inf\")\n",
        "\n",
        "        # nucleus (top-p)\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        sorted_probs, sorted_idx = torch.sort(probs, descending=True)\n",
        "        cum = torch.cumsum(sorted_probs, dim=-1)\n",
        "        mask = cum > top_p\n",
        "        mask[..., 0] = False\n",
        "        sorted_probs[mask] = 0\n",
        "        sorted_probs /= sorted_probs.sum(dim=-1, keepdim=True)\n",
        "\n",
        "        next_id = sorted_idx.gather(-1, torch.multinomial(sorted_probs, 1))\n",
        "        x = torch.cat([x, next_id], dim=1)\n",
        "\n",
        "        # early stop if tail matches any stop sequence\n",
        "        gen_ids = x[0, start_len:].tolist()\n",
        "        if gen_len + 1 >= min_tokens_before_stop:\n",
        "            if any(ends_with(gen_ids, s) for s in STOP_SEQS):\n",
        "                break\n",
        "\n",
        "    # Trim at the first stop sequence\n",
        "    gen_ids = x[0, start_len:].tolist()\n",
        "    cut_at = find_first_stop(gen_ids)\n",
        "    gen_ids = gen_ids[:cut_at]\n",
        "\n",
        "    reply = detok_cleanup(sp.decode(gen_ids))\n",
        "    if include_prompt:\n",
        "        return f\"You: {prompt.strip()}\\nBot: {reply}\"\n",
        "    return reply\n",
        "\n",
        "# ================================\n",
        "# 7) Load best (if any) and quick sample\n",
        "# ================================\n",
        "if os.path.exists(ckpt_path):\n",
        "    sd = torch.load(ckpt_path, map_location=device)\n",
        "    model.load_state_dict(sd[\"model\"])\n",
        "    print(\"Loaded best checkpoint.\")\n",
        "\n",
        "print(sample(model, sp, \"Hi, how are you?\", max_new_tokens=80, include_prompt=True))\n",
        "print(\"----\")\n",
        "print(sample(model, sp, \"What's your favorite movie?\", max_new_tokens=80, include_prompt=True))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# Chat REPL for <A>/<B>/<eot> (no class duplication)\n",
        "# ============================\n",
        "import os, torch, sentencepiece as spm\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# Paths\n",
        "sp_model_path = \"/content/spm_dd_4k.model\"\n",
        "ckpt_path     = \"/content/tinygpt_best.pt\"\n",
        "assert os.path.exists(sp_model_path), f\"Missing tokenizer at {sp_model_path}\"\n",
        "assert os.path.exists(ckpt_path), f\"Missing checkpoint at {ckpt_path}\"\n",
        "\n",
        "# Load tokenizer & special tokens\n",
        "sp = spm.SentencePieceProcessor(model_file=sp_model_path)\n",
        "ID_BOS = sp.piece_to_id(\"<bos>\")\n",
        "ID_EOS = sp.piece_to_id(\"<eos>\")\n",
        "ID_A   = sp.piece_to_id(\"<A>\")\n",
        "ID_B   = sp.piece_to_id(\"<B>\")\n",
        "ID_EOT = sp.piece_to_id(\"<eot>\")\n",
        "print(\"Special IDs:\", {\"<bos>\":ID_BOS,\"<eos>\":ID_EOS,\"<A>\":ID_A,\"<B>\":ID_B,\"<eot>\":ID_EOT})\n",
        "\n",
        "# Load checkpoint, rebuild model using your already-defined TinyGPT class\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "ckpt = torch.load(ckpt_path, map_location=device)\n",
        "cfg  = ckpt.get(\"config\", {})\n",
        "block_size = cfg.get(\"block_size\", 128)\n",
        "vocab_size = cfg.get(\"vocab_size\", sp.vocab_size())\n",
        "\n",
        "# NOTE: TinyGPT class must already exist from previous cell\n",
        "model = TinyGPT(\n",
        "    vocab_size=vocab_size,\n",
        "    d_model=256, n_heads=4, d_ff=1024, n_layers=5,\n",
        "    max_seq_len=block_size, dropout=0.1, pad_id=0\n",
        ").to(device)\n",
        "model.load_state_dict(ckpt[\"model\"])\n",
        "model.eval()\n",
        "print(\"Loaded model. Context len =\", block_size)\n",
        "\n",
        "# ---- Helpers ----\n",
        "def detok_cleanup(txt: str) -> str:\n",
        "    return (txt.replace(\" ,\", \",\").replace(\" .\", \".\").replace(\" !\", \"!\")\n",
        "              .replace(\" ?\", \"?\").replace(\" ’ \", \"’\")\n",
        "              .replace(\" ' s\",\"’s\").replace(\" ' m\",\"’m\")\n",
        "              .replace(\" ' ve\",\"’ve\").replace(\" ' re\",\"’re\")\n",
        "              .replace(\" ' d\",\"’d\").replace(\" ' ll\",\"’ll\").replace(\" n't\",\"n’t\")).strip()\n",
        "\n",
        "MAX_CTX_TOKENS = block_size\n",
        "RESERVED_GEN_TOKENS = 64  # leave room for the new reply\n",
        "\n",
        "def build_seed_from_history(history, user_msg):\n",
        "    \"\"\"\n",
        "    history: list[(user, bot)]\n",
        "    Produces:\n",
        "    <bos>\n",
        "    <A> user1 <eot>\n",
        "    <B> bot1  <eot>\n",
        "    ...\n",
        "    <A> current_user <eot>\n",
        "    <B>\n",
        "    \"\"\"\n",
        "    def convo(turns, last_user):\n",
        "        lines = []\n",
        "        for u, b in turns:\n",
        "            if u: lines.append(f\"<A> {u.strip()} <eot>\")\n",
        "            if b: lines.append(f\"<B> {b.strip()} <eot>\")\n",
        "        lines.append(f\"<A> {last_user.strip()} <eot>\")\n",
        "        return \"<bos>\\n\" + \"\\n\".join(lines) + \"\\n<B> \"\n",
        "    seed = convo(history, user_msg)\n",
        "    ids  = sp.encode(seed, out_type=int)\n",
        "    while len(ids) > (MAX_CTX_TOKENS - RESERVED_GEN_TOKENS) and history:\n",
        "        history.pop(0)\n",
        "        seed = convo(history, user_msg)\n",
        "        ids  = sp.encode(seed, out_type=int)\n",
        "    return seed\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_from_seed(seed,\n",
        "                       temperature=0.80, top_p=0.95,\n",
        "                       max_new_tokens=120, min_tokens_before_stop=24,\n",
        "                       repetition_penalty=1.15, penalty_ctx=80):\n",
        "    # Stop on end-of-turn or control tags\n",
        "    STOP_STRINGS = [\"<eot>\", \"<eos>\", \"<A>\", \"<B>\"]\n",
        "    STOP_SEQS = [sp.encode(s, out_type=int) for s in STOP_STRINGS]\n",
        "    FIRST_TOKENS = {seq[0] for seq in STOP_SEQS if len(seq) > 0}\n",
        "\n",
        "    def ends_with(seq, suffix):\n",
        "        L = len(suffix)\n",
        "        return L > 0 and len(seq) >= L and seq[-L:] == suffix\n",
        "\n",
        "    def find_first_stop(gen_ids):\n",
        "        N = len(gen_ids); cut = N\n",
        "        for i in range(N):\n",
        "            for s in STOP_SEQS:\n",
        "                L = len(s)\n",
        "                if L and i+L <= N and gen_ids[i:i+L] == s:\n",
        "                    cut = min(cut, i)\n",
        "        return cut\n",
        "\n",
        "    x = torch.tensor(sp.encode(seed, out_type=int), dtype=torch.long, device=device)[None, ...]\n",
        "    start_len = x.size(1)\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        if x.size(1) > block_size:\n",
        "            x = x[:, -block_size:]\n",
        "\n",
        "        with torch.amp.autocast(\"cuda\", enabled=(device==\"cuda\")):\n",
        "            logits = model(x)[:, -1, :]\n",
        "\n",
        "        # temperature\n",
        "        logits = logits / max(1e-8, temperature)\n",
        "\n",
        "        # repetition penalty over recent context\n",
        "        if repetition_penalty and penalty_ctx > 0:\n",
        "            recent = x[0, max(0, x.size(1)-penalty_ctx):].tolist()\n",
        "            for t in set(recent):\n",
        "                logits[0, t] /= repetition_penalty\n",
        "\n",
        "        # (optional) before min length, block starting any stop sequence\n",
        "        gen_len = x.size(1) - start_len\n",
        "        if gen_len < min_tokens_before_stop:\n",
        "            for t0 in FIRST_TOKENS:\n",
        "                logits[0, t0] = -float(\"inf\")\n",
        "\n",
        "        # nucleus (top-p)\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        sorted_probs, sorted_idx = torch.sort(probs, descending=True)\n",
        "        cum = torch.cumsum(sorted_probs, dim=-1)\n",
        "        mask = cum > top_p\n",
        "        mask[..., 0] = False\n",
        "        sorted_probs[mask] = 0\n",
        "        sorted_probs /= sorted_probs.sum(dim=-1, keepdim=True)\n",
        "\n",
        "        next_id = sorted_idx.gather(-1, torch.multinomial(sorted_probs, 1))\n",
        "        x = torch.cat([x, next_id], dim=1)\n",
        "\n",
        "        # early stop if the tail matches any stop sequence\n",
        "        gen_ids = x[0, start_len:].tolist()\n",
        "        if gen_len + 1 >= min_tokens_before_stop:\n",
        "            if any(ends_with(gen_ids, s) for s in STOP_SEQS):\n",
        "                break\n",
        "\n",
        "    # Trim at the first stop *sequence*\n",
        "    gen_ids = x[0, start_len:].tolist()\n",
        "    cut_at = find_first_stop(gen_ids)\n",
        "    gen_ids = gen_ids[:cut_at]\n",
        "\n",
        "    txt = sp.decode(gen_ids)\n",
        "    return detok_cleanup(txt)\n",
        "\n",
        "# ---- Console loop (single reply per turn, history-aware) ----\n",
        "print(\"\\nChat ready. Type 'reset' to clear history, or 'exit' to quit.\\n\")\n",
        "history = []\n",
        "while True:\n",
        "    user = input(\"You: \").strip()\n",
        "    if user.lower() in {\"exit\", \"quit\"}: break\n",
        "    if user.lower() == \"reset\":\n",
        "        history.clear(); print(\"Bot: (history cleared)\\n\"); continue\n",
        "    seed = build_seed_from_history(history.copy(), user)\n",
        "    bot  = generate_from_seed(seed)\n",
        "    print(f\"Bot: {bot}\\n\")\n",
        "    history.append((user, bot))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "id": "RzbbJXaW5N1w",
        "outputId": "8194dffc-3fa8-45c9-cefa-1c52d2660a7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Special IDs: {'<bos>': 4, '<eos>': 5, '<A>': 6, '<B>': 7, '<eot>': 8}\n",
            "Loaded model. Context len = 128\n",
            "\n",
            "Chat ready. Type 'reset' to clear history, or 'exit' to quit.\n",
            "\n",
            "You: hello\n",
            "Bot: Good, please. What’s wrong? I am sorry to do that for you? I'm on my new.\n",
            "\n",
            "You: what?\n",
            "Bot: Well, we have been interested in the phone party and need a job as possible for the room and two account. If the field will take your first, then go stop, you can play the down.\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4076509495.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     \u001b[0muser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"exit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"quit\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"reset\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    }
  ]
}